# Mini Project 4

Paper:
https://arxiv.org/pdf/1801.00318.pdf

Repository:
https://github.com/AFAgarap/malware-classification

Note that we maintain a fork here:
https://github.com/plankp/malware-classification and we will be using that one instead (in case the original repository is updated, causing the results to deviate from the original paper).

## Unofficial Plan

- Run with pre-trained weights
  - [x] Provide accuracy values
  - [x] Provide discussion of result
  - [x] Generate confusion matrix
- Visuals
  - [x] Epoch vs Accuracy (Figure 2 in original paper)
  - [x] Epoch vs Loss (not done in original paper)
  - [x] Variant where we cut of the first few epochs
- [x] Time the execution time
- Ablation study / Hyperparameter stuff
  - [x] Reduce GRU layers from 5 to 3
  - [x] Increase Regularization (SVM C) for MLP-SVM
- Additional research
  - [x] Use the [Swish](https://arxiv.org/abs/1710.05941v2) activation function

## Encoutered Difficulties

<ul>
  <li>Environment Setup</li>
  <ul>
    <li>Tensorflow Version</li>
    <p>We did not know about this <code>%tensorflow_version 1.x</code> thing, we took around half a day to figure out how to switch back to 1.15</p>
    <p>&rightarrow; Solution was to use <code>%tensorflow_version 1.x</code></p>
    <li>Setting up the repo</li>
    <p>Due to how Python imports modules, we actually had to clone into the current directory, and not into a directory nested in the current one</p>
    <p>&rightarrow; Solution make sure we pull into the working directory</p>
    <li>Dependencies Conflict</li>
    <p>For example, Colaboratory has a newer numpy version where <code>np.load</code> has a different default behaviour</p>
    <p>&rightarrow; Solution rewrite the affect code in the notebook directly</p>
    <li>Colaboratory Disconnects</li>
    <p>Seriously!</p>
    <p>&rightarrow; Solution <b><i>Keep calm and hit the "Reconnect" button</i></b></p>
  </ul>
  <li>Model</li>
  <ul>
    <li>Coding Convention</li>
    <p>For example, we see <code>penalty_parameter</code> for some models and <code>svm_c</code> for another, but they all mean the same thing</p>
    <p>&rightarrow; Solution learn the API and use the correct keyword arguments</p>
    <p>Log messages is another example of this. We see loss -- accuracy for one model and accuracy -- loss for another</p>
    <p>&rightarrow; Solution pass additional argument to process the logs accordingly</p>
    <li>Design Pattern</li>
    <p>Had to manually reset the global state of tensorflow before the model runs to prevent running into strange redefinition errors (from tensorflow)</p>
    <p>&rightarrow; Solution use <code>tf.keras.backend.clear_session()</code> to reset tensorflow's interal state</p>
    <li>Machine Learning Norm</li>
    <p>It's a bit odd to see the <b>train</b> method of a model also accepting test data.</p>
    <p>&rightarrow; Solution realize that there was another piece of code in the original repository that will collect the results and use that in conjunction</p>
  </ul>
</ul>

> **Notes**
>
> *  We are not sure about GRU due to lack of exposure, but the model appears to be correct.
> *  Tensorflow gives lots of warning messages (due to deprecated API usage)


## Setup Environment

To fetch the code:


```python
%%shell

# clone the code into the current directory
git init .
git remote add origin https://github.com/plankp/malware-classification
git pull origin master
```

    Initialized empty Git repository in /content/.git/
    remote: Enumerating objects: 438, done.[K
    remote: Counting objects: 100% (113/113), done.[K
    remote: Compressing objects: 100% (94/94), done.[K
    remote: Total 438 (delta 34), reused 73 (delta 14), pack-reused 325[K
    Receiving objects: 100% (438/438), 108.62 MiB | 35.20 MiB/s, done.
    Resolving deltas: 100% (183/183), done.
    From https://github.com/plankp/malware-classification
     * branch            master     -> FETCH_HEAD
     * [new branch]      master     -> origin/master





    



We also need to report the environment we use. Here we have tensorflow list all the devices we have access to.


```python
%tensorflow_version 1.x

import tensorflow as tf
print('Tensorflow Version:')
print(tf.version.VERSION)
print()

print('Devices:')
for dev in tf.config.experimental_list_devices():
  print(dev)
```

    Tensorflow Version:
    1.15.2
    
    Devices:
    /job:localhost/replica:0/task:0/device:CPU:0
    /job:localhost/replica:0/task:0/device:XLA_CPU:0
    /job:localhost/replica:0/task:0/device:XLA_GPU:0
    /job:localhost/replica:0/task:0/device:GPU:0


As for CPU information:


```python
!cat /proc/cpuinfo
```

    processor	: 0
    vendor_id	: GenuineIntel
    cpu family	: 6
    model		: 79
    model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
    stepping	: 0
    microcode	: 0x1
    cpu MHz		: 2199.998
    cache size	: 56320 KB
    physical id	: 0
    siblings	: 2
    core id		: 0
    cpu cores	: 1
    apicid		: 0
    initial apicid	: 0
    fpu		: yes
    fpu_exception	: yes
    cpuid level	: 13
    wp		: yes
    flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
    bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
    bogomips	: 4399.99
    clflush size	: 64
    cache_alignment	: 64
    address sizes	: 46 bits physical, 48 bits virtual
    power management:
    
    processor	: 1
    vendor_id	: GenuineIntel
    cpu family	: 6
    model		: 79
    model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
    stepping	: 0
    microcode	: 0x1
    cpu MHz		: 2199.998
    cache size	: 56320 KB
    physical id	: 0
    siblings	: 2
    core id		: 0
    cpu cores	: 1
    apicid		: 1
    initial apicid	: 1
    fpu		: yes
    fpu_exception	: yes
    cpuid level	: 13
    wp		: yes
    flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
    bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
    bogomips	: 4399.99
    clflush size	: 64
    cache_alignment	: 64
    address sizes	: 46 bits physical, 48 bits virtual
    power management:
    


And GPU information:

> **Note**
>
> This might change depending on how Colaboratory decides to allocate its resources. Over the past several runs, we have came across a Tesla T4, NVIDIA K80, ... etc.


```python
!nvidia-smi -L
```

    GPU 0: Tesla T4 (UUID: GPU-a813b581-6904-4c32-b2e9-c22e9c34ac3e)


# Code Patching

This section is only needed if any of the provided code uses (then-deprecated) now-removed features.

Note that `main.py` and `classifier.py` rely on outdated behavior of `np.load`, but since we do not use those files, patching is not needed.

# Process Malimg Dataset

In this section, we load the Malimg dataset provided by the author and split it into train and test instances using a 70-30 split â€“ the same scheme used by the author.

One modification we make is to pass the `random_state` keyword argument when splitting. That way we can guarantee the train and test data is the same everytime the notebook is executed.

some stuff that we need for presenting the results:


```python
import pandas as pd
import matplotlib.pyplot as plt
from utils.data import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from time import time

MALWARE_FAMILIES = [
    "Adialer.C",
    "Agent.FYI",
    "Allaple.A",
    "Allaple.L",
    "Alueron.gen!J",
    "Autorun.K",
    "C2LOP.P",
    "C2LOP.gen!g",
    "Dialplatform.B",
    "Dontovo.A",
    "Fakerean",
    "Instantaccess",
    "Lolyda.AA1",
    "Lolyda.AA2",
    "Lolyda.AA3",
    "Lolyda.AT",
    "Malex.gen!J",
    "Obfuscator.AD",
    "Rbot!gen",
    "Skintrim.N",
    "Swizzor.gen!E",
    "Swizzor.gen!I",
    "VB.AT",
    "Wintrim.BX",
    "Yuner.A",
]

print('DONE')
```

    DONE


load the dataset and present some statistics:


```python
import numpy as np
from sklearn.model_selection import train_test_split
from utils.data import load_data
from utils.data import one_hot_encode

BATCH_SIZE = 256

malimg_dataset = np.load('./dataset/malimg.npz', allow_pickle=True)
features, labels = load_data(dataset=malimg_dataset)

print(f'Total number of samples prior to split: {features.shape[0]}')
df = pd.DataFrame({
    'Occurrences': np.bincount(labels),
    'Percecntage': np.bincount(labels) / features.shape[0] * 100
}, index=MALWARE_FAMILIES)
display(df.T)
print('\nBar Chart:')
df.iloc[0:].plot.bar()
plt.show()
print('\nPie Chart:')
plt.figure(figsize=(8, 8))
plt.pie(np.bincount(labels), labels=MALWARE_FAMILIES)
plt.show()
del df
print()

labels = one_hot_encode(labels=labels)
```

    Total number of samples prior to split: 9339



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Adialer.C</th>
      <th>Agent.FYI</th>
      <th>Allaple.A</th>
      <th>Allaple.L</th>
      <th>Alueron.gen!J</th>
      <th>Autorun.K</th>
      <th>C2LOP.P</th>
      <th>C2LOP.gen!g</th>
      <th>Dialplatform.B</th>
      <th>Dontovo.A</th>
      <th>Fakerean</th>
      <th>Instantaccess</th>
      <th>Lolyda.AA1</th>
      <th>Lolyda.AA2</th>
      <th>Lolyda.AA3</th>
      <th>Lolyda.AT</th>
      <th>Malex.gen!J</th>
      <th>Obfuscator.AD</th>
      <th>Rbot!gen</th>
      <th>Skintrim.N</th>
      <th>Swizzor.gen!E</th>
      <th>Swizzor.gen!I</th>
      <th>VB.AT</th>
      <th>Wintrim.BX</th>
      <th>Yuner.A</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Occurrences</th>
      <td>122.00000</td>
      <td>116.000000</td>
      <td>2949.000000</td>
      <td>1591.000000</td>
      <td>198.000000</td>
      <td>106.000000</td>
      <td>200.000000</td>
      <td>146.000000</td>
      <td>177.000000</td>
      <td>162.000000</td>
      <td>381.000000</td>
      <td>431.000000</td>
      <td>213.000000</td>
      <td>184.000000</td>
      <td>123.000000</td>
      <td>159.000000</td>
      <td>136.000000</td>
      <td>142.000000</td>
      <td>158.00000</td>
      <td>80.000000</td>
      <td>128.000000</td>
      <td>132.000000</td>
      <td>408.000000</td>
      <td>97.000000</td>
      <td>800.000000</td>
    </tr>
    <tr>
      <th>Percecntage</th>
      <td>1.30635</td>
      <td>1.242103</td>
      <td>31.577257</td>
      <td>17.036085</td>
      <td>2.120141</td>
      <td>1.135025</td>
      <td>2.141557</td>
      <td>1.563337</td>
      <td>1.895278</td>
      <td>1.734661</td>
      <td>4.079666</td>
      <td>4.615055</td>
      <td>2.280758</td>
      <td>1.970232</td>
      <td>1.317058</td>
      <td>1.702538</td>
      <td>1.456259</td>
      <td>1.520505</td>
      <td>1.69183</td>
      <td>0.856623</td>
      <td>1.370596</td>
      <td>1.413428</td>
      <td>4.368776</td>
      <td>1.038655</td>
      <td>8.566228</td>
    </tr>
  </tbody>
</table>
</div>


    
    Bar Chart:



    
![png](images/output_21_3.png)
    


    
    Pie Chart:



    
![png](images/output_21_5.png)
    


    


and we split the data:


```python
# Here we load the Malimg dataset and split it into train and test data using a
# 70-30 scheme (same scheme used in `main.py`)

num_features = features.shape[1]
num_classes = labels.shape[1]

train_features, test_features, train_labels, test_labels = train_test_split(
    features, labels, test_size=0.30, stratify=labels,
    random_state=1234   # <- added to have consistent splits
)

train_size = train_features.shape[0]
train_features = train_features[: train_size - (train_size % BATCH_SIZE)]
train_labels = train_labels[: train_size - (train_size % BATCH_SIZE)]
train_size = train_features.shape[0]

test_size = test_features.shape[0]
test_features = test_features[: test_size - (test_size % BATCH_SIZE)]
test_labels = test_labels[: test_size - (test_size % BATCH_SIZE)]
test_size = test_features.shape[0]

print(f'Number of features: {num_features}')
print(f'Number of classes:  {num_classes}')
print(f'Train size:         {train_size}')
print(f'Test size:          {test_size}')
```

    Number of features: 1024
    Number of classes:  25
    Train size:         6400
    Test size:          2560


# Reproduce Results

In this section, we will run the three models â€” CNN-SVM, GRU-SVM and MLP-SVM â€” against the splitted data. We expect to get performance similar to the ones presented in the report.

We will be using the pretrained models provided by the author as well as completely retraining the model with the configurations presented in the report.

## Pretrained Models

Since the original author has provided the pretrained models in the repository, we will be using those to see if we can get similar performance as the report.

Despite having obtained much higher accuracies, these values follow the same trend as the report â€” GRU > MLP > CNN.

We suspect this is due to the way the dataset is split. The author uses `train_test_split` but does not report the seed used for the randomizer state. With the seed we pick, we may be including some of the training instances from the partition boundaries obtained by the author.

This serves as one of the reasons for us to retrain all three models with the configurations presented in the report.

### Utilities


```python
def present_results(phase, predictions):
  predictions_ = np.reshape(predictions, (predictions.shape[0] // num_classes,
                                          num_classes))
  test_labels_ = None

  with tf.Session() as sess:
    predictions_ = sess.run(tf.argmax(predictions_, 1))
    test_labels_ = sess.run(tf.argmax(test_labels, 1))

  # get the confusion matrix based on the actual and predicted labels
  conf = confusion_matrix(y_true=test_labels_, y_pred=predictions_)

  # get the classification report on the actual and predicted labels
  report = classification_report(
      y_true=test_labels_, y_pred=predictions_, target_names=MALWARE_FAMILIES
  )

  # create a confusion matrix plot
  plt.imshow(conf, cmap=plt.cm.Greys, interpolation="nearest")

  # set the plot title
  plt.title("Confusion Matrix for {} Phase".format(phase))

  # legend of intensity for the plot
  plt.colorbar()

  tick_marks = np.arange(len(MALWARE_FAMILIES))
  plt.xticks(tick_marks, MALWARE_FAMILIES, rotation=45)
  plt.yticks(tick_marks, MALWARE_FAMILIES)

  plt.tight_layout()
  plt.ylabel("Actual label")
  plt.xlabel("Predicted label")

  # show the plot
  plt.show()

  return conf, report
```

### CNN-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()
predictions, accuracies = predict(dataset=[test_features, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  model=1,  # <- 1 is for CNN-SVM
                                  model_path='./trained-cnn-svm')

display(predictions)
display(np.mean(accuracies))
```

    WARNING:tensorflow:From /content/classifier.py:45: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.
    
    WARNING:tensorflow:From /content/classifier.py:45: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.
    
    WARNING:tensorflow:From /content/classifier.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    
    WARNING:tensorflow:From /content/classifier.py:98: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.
    
    INFO:tensorflow:Restoring parameters from ./trained-cnn-svm/CNN-SVM-2400
    Loaded trained model from ./trained-cnn-svm/CNN-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.942578125



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained CNN-SVM', predictions)
```


    
![png](images/output_32_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11</td>
      <td>0</td>
      <td>772</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12</td>
      <td>0</td>
      <td>76</td>
      <td>345</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>105</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>48</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>2</td>
      <td>0</td>
      <td>8</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>44</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>7</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>25</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>27</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>111</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>25</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.40      1.00      0.57        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.89      0.95      0.92       814
         Allaple.L       0.91      0.80      0.85       433
     Alueron.gen!J       0.98      1.00      0.99        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.98      0.81      0.89        53
       C2LOP.gen!g       0.97      0.86      0.91        36
    Dialplatform.B       1.00      1.00      1.00        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       1.00      0.98      0.99       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      1.00      1.00        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      0.91      0.95        46
       Malex.gen!J       1.00      0.68      0.81        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       0.98      0.98      0.98        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.89      0.68      0.77        37
     Swizzor.gen!I       0.90      0.75      0.82        36
             VB.AT       1.00      0.99      1.00       112
        Wintrim.BX       1.00      0.89      0.94        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.93      2560
         macro avg       0.96      0.93      0.94      2560
      weighted avg       0.94      0.93      0.93      2560
    


### GRU-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()

test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

predictions, accuracies = predict(dataset=[test_features_, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  cell_size=256,
                                  model=2,  # <- 2 is for GRU-SVM
                                  model_path='./trained-gru-svm')

display(predictions)
display(np.mean(accuracies))

del test_features_
```

    INFO:tensorflow:Restoring parameters from ./trained-gru-svm/GRU-SVM-2400
    Loaded trained model from ./trained-gru-svm/GRU-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.957421875



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained GRU-SVM', predictions)
```


    
![png](images/output_36_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>771</td>
      <td>38</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>32</td>
      <td>398</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>41</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>28</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>46</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>45</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>112</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>27</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.73      1.00      0.85        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.95      0.95      0.95       814
         Allaple.L       0.91      0.92      0.91       433
     Alueron.gen!J       0.98      1.00      0.99        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.91      0.77      0.84        53
       C2LOP.gen!g       0.93      0.78      0.85        36
    Dialplatform.B       0.98      1.00      0.99        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       0.98      0.99      0.99       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      0.98      0.99        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      1.00      1.00        46
       Malex.gen!J       0.95      0.97      0.96        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       1.00      1.00      1.00        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.89      0.84      0.86        37
     Swizzor.gen!I       0.89      0.92      0.90        36
             VB.AT       0.99      1.00      1.00       112
        Wintrim.BX       1.00      0.96      0.98        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.96      2560
         macro avg       0.96      0.96      0.96      2560
      weighted avg       0.96      0.96      0.96      2560
    


### MLP-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()
predictions, accuracies = predict(dataset=[test_features, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  model=3,  # <- 3 is for MLP-SVM
                                  model_path='./trained-mlp-svm')

display(predictions)
display(np.mean(accuracies))
```

    INFO:tensorflow:Restoring parameters from ./trained-mlp-svm/MLP-SVM-2400
    Loaded trained model from ./trained-mlp-svm/MLP-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.94765625



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained MLP-SVM', predictions)
```


    
![png](images/output_40_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21</td>
      <td>0</td>
      <td>759</td>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10</td>
      <td>0</td>
      <td>27</td>
      <td>396</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>38</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>11</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>21</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>29</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>10</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>110</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>26</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.29      1.00      0.46        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.94      0.93      0.94       814
         Allaple.L       0.92      0.91      0.92       433
     Alueron.gen!J       0.98      0.96      0.97        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.86      0.72      0.78        53
       C2LOP.gen!g       1.00      0.58      0.74        36
    Dialplatform.B       1.00      1.00      1.00        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       1.00      0.99      1.00       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      0.98      0.99        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      0.93      0.97        46
       Malex.gen!J       1.00      0.76      0.87        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       1.00      0.93      0.97        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.96      0.68      0.79        37
     Swizzor.gen!I       1.00      0.72      0.84        36
             VB.AT       1.00      0.98      0.99       112
        Wintrim.BX       1.00      0.93      0.96        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.93      2560
         macro avg       0.96      0.92      0.93      2560
      weighted avg       0.96      0.93      0.94      2560
    


## Retrained Models

In this section, we will be retraining the three models, and see if we can get similar performance as the report.

The values are similar to the ones presented in the report.

### Utilities


```python
import re

def process_capture(cap, model_type):
  """
  Returns:
  (steps, losses, train-accuracies)
  """

  cap.show()

  train_steps = []
  train_loss = []
  train_acc = []

  pattern = None
  grp_step = None
  grp_loss = None
  grp_acc = None
  if model_type == 1:
    # step: 0, training accuracy : 0.0859375, training loss : 1936.3583984375
    grp_step = 1
    grp_loss = 3
    grp_acc = 2
    pattern = r'^step: (.*), training accuracy : (.*), training loss : (.*)$'
  else:
    # step [0] train -- loss : 9.729708671569824, accuracy : 0.359375
    grp_step = 1
    grp_loss = 2
    grp_acc = 3
    pattern = r'^step \[(.*)\] train -- loss : (.*), accuracy : (.*)$'


  for line in cap.stdout.splitlines():
    if line.startswith('step'):
      groups = re.search(pattern, line)
      train_steps.append(int(groups.group(grp_step)))
      train_loss.append(float(groups.group(grp_loss)))
      train_acc.append(float(groups.group(grp_acc)))
    elif line.startswith('EOF'):
      break

  return train_steps, train_loss, train_acc
```

### CNN-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm
!mkdir ./repr_cnn_svm

from models.cnn_svm import CNN

tf.keras.backend.clear_session()
model = CNN(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            sequence_length=num_features,
            alpha=1e-3,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm/checkpoints/',
            log_path='./repr_cnn_svm/logs/',
            result_path='./repr_cnn_svm/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```

    WARNING:tensorflow:From /content/models/cnn_svm.py:52: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:312: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:342: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
    WARNING:tensorflow:From /content/models/cnn_svm.py:109: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:111: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
    
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    WARNING:tensorflow:From /content/models/cnn_svm.py:124: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:172: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:178: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:180: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
    
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use standard file APIs to delete files with this prefix.



```python
cnn_steps, cnn_losses, cnn_accs = process_capture(model_type=1, # 1 <- CNN
                                                  cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.171875, training loss : 1885.4095458984375
    step: 100, training accuracy : 0.6875, training loss : 3.1581640243530273
    step: 200, training accuracy : 0.75, training loss : 1.8312004804611206
    step: 300, training accuracy : 0.78125, training loss : 1.2454259395599365
    step: 400, training accuracy : 0.80078125, training loss : 1.1658823490142822
    step: 500, training accuracy : 0.82421875, training loss : 0.8336452841758728
    step: 600, training accuracy : 0.84375, training loss : 0.6880810856819153
    step: 700, training accuracy : 0.87109375, training loss : 0.6321308016777039
    step: 800, training accuracy : 0.87109375, training loss : 0.4600289463996887
    step: 900, training accuracy : 0.90234375, training loss : 0.4091877341270447
    step: 1000, training accuracy : 0.92578125, training loss : 0.3934893012046814
    step: 1100, training accuracy : 0.953125, training loss : 0.31840550899505615
    step: 1200, training accuracy : 0.97265625, training loss : 0.24278147518634796
    step: 1300, training accuracy : 0.98046875, training loss : 0.26451587677001953
    step: 1400, training accuracy : 0.9921875, training loss : 0.17115572094917297
    step: 1500, training accuracy : 1.0, training loss : 0.17136894166469574
    step: 1600, training accuracy : 0.9921875, training loss : 0.1229921206831932
    step: 1700, training accuracy : 1.0, training loss : 0.10592841356992722
    step: 1800, training accuracy : 0.94921875, training loss : 0.2217751145362854
    step: 1900, training accuracy : 1.0, training loss : 0.09413359314203262
    step: 2000, training accuracy : 1.0, training loss : 0.09087356925010681
    step: 2100, training accuracy : 1.0, training loss : 0.043686188757419586
    step: 2200, training accuracy : 1.0, training loss : 0.06267675757408142
    step: 2300, training accuracy : 1.0, training loss : 0.06763534992933273
    step: 2400, training accuracy : 1.0, training loss : 0.07296913862228394
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 100, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 200, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 300, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 400, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 500, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 600, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 700, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 800, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    step: 900, testing accuracy : 0.78515625, testing loss : 1.1242585182189941
    EOF -- Testing done at step 999
    
    Completed after: 47.70s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM', './repr_cnn_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_49_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM', report))
print("{} Accuracy : {}".format('CNN-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10900</td>
      <td>0</td>
      <td>58100</td>
      <td>12300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5900</td>
      <td>0</td>
      <td>6500</td>
      <td>30900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3800</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1300</td>
      <td>0</td>
      <td>1800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1400</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10800</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.10      1.00      0.18      3300
         Agent.FYI       1.00      0.97      0.98      3100
         Allaple.A       0.86      0.71      0.78     81400
         Allaple.L       0.71      0.71      0.71     43300
     Alueron.gen!J       1.00      0.94      0.97      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.54      0.13      0.21      5300
       C2LOP.gen!g       0.41      0.19      0.26      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.99      0.95      0.97     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       0.97      0.83      0.89      4600
       Malex.gen!J       0.67      0.16      0.26      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.89      0.94      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.58      0.19      0.29      3700
     Swizzor.gen!I       0.50      0.14      0.22      3600
             VB.AT       0.99      0.96      0.98     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.78    256000
         macro avg       0.85      0.78      0.78    256000
      weighted avg       0.86      0.78      0.80    256000
    
    CNN-SVM Accuracy : 0.78125


### GRU-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm
!mkdir ./repr_gru_svm

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=5,
               dropout_rate=0.85,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm/checkpoints/',
            log_path='./repr_gru_svm/logs/',
            result_path='./repr_gru_svm/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    WARNING:tensorflow:
    The TensorFlow contrib module will not be included in TensorFlow 2.0.
    For more information, please see:
      * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
      * https://github.com/tensorflow/addons
      * https://github.com/tensorflow/io (for I/O related ops)
    If you depend on functionality not listed there, please file an issue.
    
    WARNING:tensorflow:From /content/models/gru_svm.py:99: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
    WARNING:tensorflow:From /content/models/gru_svm.py:105: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
    WARNING:tensorflow:From /content/models/gru_svm.py:113: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `keras.layers.RNN(cell)`, which is equivalent to this API
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `layer.add_weight` method instead.
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    WARNING:tensorflow:From /content/models/gru_svm.py:121: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.
    
    WARNING:tensorflow:From /content/models/gru_svm.py:397: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.
    


    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm/results/training-* # we capture the output for this info

gru_steps, gru_losses, gru_accs = process_capture(model_type=2, # 2 <- GRU
                                                  cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.287469863891602, accuracy : 0.3828125
    step [100] train -- loss : 0.6991578936576843, accuracy : 0.69921875
    step [200] train -- loss : 0.4392330050468445, accuracy : 0.828125
    step [300] train -- loss : 0.33608123660087585, accuracy : 0.8984375
    step [400] train -- loss : 0.19293802976608276, accuracy : 0.9375
    step [500] train -- loss : 0.16819651424884796, accuracy : 0.96484375
    step [600] train -- loss : 0.08585874736309052, accuracy : 0.984375
    step [700] train -- loss : 0.08715782314538956, accuracy : 0.98046875
    step [800] train -- loss : 0.030301596969366074, accuracy : 0.99609375
    step [900] train -- loss : 0.044020939618349075, accuracy : 0.98828125
    step [1000] train -- loss : 0.02474997192621231, accuracy : 0.99609375
    step [1100] train -- loss : 0.01299608126282692, accuracy : 1.0
    step [1200] train -- loss : 0.032622069120407104, accuracy : 0.98828125
    step [1300] train -- loss : 0.020846670493483543, accuracy : 0.99609375
    step [1400] train -- loss : 0.027041319757699966, accuracy : 0.9921875
    step [1500] train -- loss : 0.01581275835633278, accuracy : 0.99609375
    step [1600] train -- loss : 0.008078668266534805, accuracy : 1.0
    step [1700] train -- loss : 0.018754320219159126, accuracy : 0.99609375
    step [1800] train -- loss : 0.017474502325057983, accuracy : 0.99609375
    step [1900] train -- loss : 0.030146373435854912, accuracy : 0.9921875
    step [2000] train -- loss : 0.010028847493231297, accuracy : 1.0
    step [2100] train -- loss : 0.021390773355960846, accuracy : 0.99609375
    step [2200] train -- loss : 0.014722506515681744, accuracy : 0.99609375
    step [2300] train -- loss : 0.013401571661233902, accuracy : 0.99609375
    step [2400] train -- loss : 0.009496651589870453, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [200] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [300] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [400] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [500] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [600] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [700] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [800] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    step [900] test -- loss : 0.6911402344703674, accuracy : 0.85546875
    EOF -- Testing done at step 999
    
    Completed after: 279.69s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('GRU-SVM', './repr_gru_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_54_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM', report))
print("{} Accuracy : {}".format('GRU-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100</td>
      <td>0</td>
      <td>69100</td>
      <td>12000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>200</td>
      <td>0</td>
      <td>9600</td>
      <td>33400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>400</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>3500</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>200</td>
      <td>0</td>
      <td>300</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>800</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>1400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>1000</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.70      1.00      0.82      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.85      0.85      0.85     81400
         Allaple.L       0.73      0.77      0.75     43300
     Alueron.gen!J       0.86      1.00      0.92      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.64      0.66      0.65      5300
       C2LOP.gen!g       0.62      0.50      0.55      3600
    Dialplatform.B       0.98      1.00      0.99      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.98      0.98      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.95      0.98      0.97      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.89      0.94      4600
       Malex.gen!J       0.93      0.74      0.82      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.98      0.98      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.43      0.27      0.33      3700
     Swizzor.gen!I       0.38      0.28      0.32      3600
             VB.AT       0.95      1.00      0.97     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.87    256000
         macro avg       0.88      0.87      0.87    256000
      weighted avg       0.87      0.87      0.87    256000
    
    GRU-SVM Accuracy : 0.86796875


### MLP-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm
!mkdir ./repr_mlp_svm

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-3,
            penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm/checkpoints/',
            log_path='./repr_mlp_svm/logs/',
            result_path='./repr_mlp_svm/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_steps, mlp_losses, mlp_accs = process_capture(model_type=3, # 3 <- MLP
                                                  cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.017770035192370415, accuracy : 0.98828125
    step [200] train -- loss : 0.013183005154132843, accuracy : 0.99609375
    step [300] train -- loss : 0.009883206337690353, accuracy : 1.0
    step [400] train -- loss : 0.008431791327893734, accuracy : 1.0
    step [500] train -- loss : 0.0072814784944057465, accuracy : 1.0
    step [600] train -- loss : 0.006273379549384117, accuracy : 1.0
    step [700] train -- loss : 0.005399349611252546, accuracy : 1.0
    step [800] train -- loss : 0.00464605400338769, accuracy : 1.0
    step [900] train -- loss : 0.004017016850411892, accuracy : 1.0
    step [1000] train -- loss : 0.0034515128936618567, accuracy : 1.0
    step [1100] train -- loss : 0.002975847339257598, accuracy : 1.0
    step [1200] train -- loss : 0.0025619384832680225, accuracy : 1.0
    step [1300] train -- loss : 0.002207060344517231, accuracy : 1.0
    step [1400] train -- loss : 0.0019177227513864636, accuracy : 1.0
    step [1500] train -- loss : 0.0018293210305273533, accuracy : 1.0
    step [1600] train -- loss : 0.0015587080270051956, accuracy : 1.0
    step [1700] train -- loss : 0.0013019400648772717, accuracy : 1.0
    step [1800] train -- loss : 0.001165925175882876, accuracy : 1.0
    step [1900] train -- loss : 0.0009830903727561235, accuracy : 1.0
    step [2000] train -- loss : 0.0008553820662200451, accuracy : 1.0
    step [2100] train -- loss : 0.0007579507655464113, accuracy : 1.0
    step [2200] train -- loss : 0.0006648997077718377, accuracy : 1.0
    step [2300] train -- loss : 0.0005795421893708408, accuracy : 1.0
    step [2400] train -- loss : 0.0005096268141642213, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [200] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [300] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [400] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [500] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [600] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [700] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [800] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    step [900] test -- loss : 0.028227539733052254, accuracy : 0.80078125
    EOF -- Testing done at step 999
    
    Completed after: 10.13s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM', './repr_mlp_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_59_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM', report))
print("{} Accuracy : {}".format('MLP-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3100</td>
      <td>0</td>
      <td>70400</td>
      <td>7800</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2400</td>
      <td>0</td>
      <td>14200</td>
      <td>26700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2600</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1800</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>2400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2400</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>800</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10700</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.16      1.00      0.28      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.77      0.86      0.82     81400
         Allaple.L       0.77      0.62      0.68     43300
     Alueron.gen!J       1.00      0.84      0.91      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.32      0.15      0.21      5300
       C2LOP.gen!g       0.00      0.00      0.00      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.98      0.99      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       0.97      0.85      0.91      4600
       Malex.gen!J       1.00      0.18      0.31      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.91      0.94      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.75      0.08      0.15      3700
     Swizzor.gen!I       0.73      0.22      0.34      3600
             VB.AT       0.97      0.96      0.96     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.81    256000
         macro avg       0.86      0.77      0.77    256000
      weighted avg       0.84      0.81      0.81    256000
    
    MLP-SVM Accuracy : 0.809765625


### Visuals

In this section, we present two graphs.

The first one is the same as _Figure 2_ presented in the original paper, which plots the epoch (time step) against training accuracy.

Our graph follows a similar trend as the one presented in the original paper where MLP-SVM stays on top, and GRU-SVM is sandwiched between MLP and CNN-SVM.


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='CNN-SVM')
plt.plot(gru_steps, gru_accs, label='GRU-SVM')
plt.plot(mlp_steps, mlp_accs, label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='CNN-SVM')
plt.plot(gru_steps[3:], gru_accs[3:], label='GRU-SVM')
plt.plot(mlp_steps[2:], mlp_accs[2:], label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_64_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_64_3.png)
    


The second one plots epoch against training loss.


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='CNN-SVM')
plt.plot(gru_steps, gru_losses, label='GRU-SVM')
plt.plot(mlp_steps, mlp_losses, label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='CNN-SVM')
plt.plot(gru_steps[3:], gru_losses[3:], label='GRU-SVM')
plt.plot(mlp_steps[2:], mlp_losses[2:], label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_66_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_66_3.png)
    


# Ablation Studies and Hyperparameter Tuning

## GRU-SVM Model with 3 layers

One of the things to note is that the GRU-SVM model had the most amount of hidden layers (5 vs 2 for CNN-SVM and 3 for MLP-SVM).

The last paragraph of the discussion section suggests making CNN-SVM and MLP-SVM more complex may potentially bring their test performance closer to GRU-SVM.

Here we do the opposite, so we will be reducing the complexity of the GRU-SVM model such that it uses 3 layers instead of 5.


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_d3
!mkdir ./repr_gru_svm_d3

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=3,
               dropout_rate=0.85,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_d3/checkpoints/',
            log_path='./repr_gru_svm_d3/logs/',
            result_path='./repr_gru_svm_d3/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_d3/results/training-*  # we capture the output for this info

gru_d3_steps, gru_d3_losses, gru_d3_accs = process_capture(model_type=2, # 2 <- GRU
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.390305519104004, accuracy : 0.24609375
    step [100] train -- loss : 0.6102556586265564, accuracy : 0.69921875
    step [200] train -- loss : 0.3920157551765442, accuracy : 0.828125
    step [300] train -- loss : 0.2954268157482147, accuracy : 0.89453125
    step [400] train -- loss : 0.17756865918636322, accuracy : 0.94921875
    step [500] train -- loss : 0.12318743020296097, accuracy : 0.97265625
    step [600] train -- loss : 0.07590770721435547, accuracy : 0.984375
    step [700] train -- loss : 0.04535472393035889, accuracy : 0.9921875
    step [800] train -- loss : 0.07772169262170792, accuracy : 0.98046875
    step [900] train -- loss : 0.05630836635828018, accuracy : 0.98828125
    step [1000] train -- loss : 0.017477205023169518, accuracy : 1.0
    step [1100] train -- loss : 0.012174970470368862, accuracy : 1.0
    step [1200] train -- loss : 0.016107503324747086, accuracy : 0.99609375
    step [1300] train -- loss : 0.022550154477357864, accuracy : 0.9921875
    step [1400] train -- loss : 0.01774832233786583, accuracy : 1.0
    step [1500] train -- loss : 0.01819266378879547, accuracy : 0.99609375
    step [1600] train -- loss : 0.014382485300302505, accuracy : 0.99609375
    step [1700] train -- loss : 0.01288563571870327, accuracy : 1.0
    step [1800] train -- loss : 0.016394570469856262, accuracy : 0.99609375
    step [1900] train -- loss : 0.037675391882658005, accuracy : 0.99609375
    step [2000] train -- loss : 0.02765517123043537, accuracy : 0.9921875
    step [2100] train -- loss : 0.017384441569447517, accuracy : 0.99609375
    step [2200] train -- loss : 0.023805981501936913, accuracy : 0.9921875
    step [2300] train -- loss : 0.009719944559037685, accuracy : 1.0
    step [2400] train -- loss : 0.04205986484885216, accuracy : 0.99609375
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [200] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [300] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [400] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [500] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [600] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [700] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [800] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    step [900] test -- loss : 0.8362244367599487, accuracy : 0.85546875
    EOF -- Testing done at step 999
    
    Completed after: 185.49s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('3 Layer GRU-SVM', './repr_gru_svm_d3/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_72_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('3 Layer GRU-SVM', report))
print("{} Accuracy : {}".format('3 Layer GRU-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2200</td>
      <td>0</td>
      <td>63100</td>
      <td>15000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1500</td>
      <td>0</td>
      <td>6500</td>
      <td>35100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>400</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>3600</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>1000</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>200</td>
      <td>0</td>
      <td>1300</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1400</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>1500</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    3 Layer GRU-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.34      1.00      0.50      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.88      0.78      0.82     81400
         Allaple.L       0.70      0.81      0.75     43300
     Alueron.gen!J       0.92      0.98      0.95      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.51      0.68      0.59      5300
       C2LOP.gen!g       0.71      0.56      0.63      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.97      0.98      0.97      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       1.00      0.91      0.95      4600
       Malex.gen!J       0.62      0.47      0.54      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.96      0.97      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.29      0.05      0.09      3700
     Swizzor.gen!I       0.50      0.42      0.45      3600
             VB.AT       0.99      1.00      1.00     11200
        Wintrim.BX       0.88      0.82      0.85      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.85    256000
         macro avg       0.85      0.85      0.84    256000
      weighted avg       0.86      0.85      0.85    256000
    
    3 Layer GRU-SVM Accuracy : 0.846484375



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_accs, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_accs, label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_accs[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_accs[3:], label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_74_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_74_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_losses, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_losses, label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_losses[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_losses[3:], label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_75_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_75_3.png)
    


## MLP-SVM Model with Penalization Factor of 10

Another thing to note in the original paper is that the penalization factor (SVM C in _Table 2_) for the MLP-SVM model is much smaller compared to the two other models (0.5 vs 10).

Here we rerun the MLP-SVM model with the penalization factor of 10.


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_c10
!mkdir ./repr_mlp_svm_c10

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-3,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_c10/checkpoints/',
            log_path='./repr_mlp_svm_c10/logs/',
            result_path='./repr_mlp_svm_c10/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_c10_steps, mlp_c10_losses, mlp_c10_accs = process_capture(model_type=3, # 3 <- MLP
                                                              cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.11142793297767639, accuracy : 0.98828125
    step [200] train -- loss : 0.09832973033189774, accuracy : 0.99609375
    step [300] train -- loss : 0.01324493158608675, accuracy : 1.0
    step [400] train -- loss : 0.01262040063738823, accuracy : 1.0
    step [500] train -- loss : 0.01241095270961523, accuracy : 1.0
    step [600] train -- loss : 0.012182340957224369, accuracy : 1.0
    step [700] train -- loss : 0.011937863193452358, accuracy : 1.0
    step [800] train -- loss : 0.011680417694151402, accuracy : 1.0
    step [900] train -- loss : 0.01141244638711214, accuracy : 1.0
    step [1000] train -- loss : 0.011136241257190704, accuracy : 1.0
    step [1100] train -- loss : 0.010853770188987255, accuracy : 1.0
    step [1200] train -- loss : 0.010567124001681805, accuracy : 1.0
    step [1300] train -- loss : 0.01027743425220251, accuracy : 1.0
    step [1400] train -- loss : 0.009987159632146358, accuracy : 1.0
    step [1500] train -- loss : 0.009696323424577713, accuracy : 1.0
    step [1600] train -- loss : 0.009405172429978848, accuracy : 1.0
    step [1700] train -- loss : 0.009115809574723244, accuracy : 1.0
    step [1800] train -- loss : 0.008829891681671143, accuracy : 1.0
    step [1900] train -- loss : 0.00857170857489109, accuracy : 1.0
    step [2000] train -- loss : 0.008508719503879547, accuracy : 1.0
    step [2100] train -- loss : 0.008036244660615921, accuracy : 1.0
    step [2200] train -- loss : 0.007756669539958239, accuracy : 1.0
    step [2300] train -- loss : 0.007494267541915178, accuracy : 1.0
    step [2400] train -- loss : 0.007218551356345415, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [200] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [300] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [400] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [500] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [600] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [700] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [800] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    step [900] test -- loss : 0.5989102721214294, accuracy : 0.7890625
    EOF -- Testing done at step 999
    
    Completed after: 10.26s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM C10', './repr_mlp_svm_c10/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_80_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM C10', report))
print("{} Accuracy : {}".format('MLP-SVM C10', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4500</td>
      <td>0</td>
      <td>66800</td>
      <td>10000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4700</td>
      <td>0</td>
      <td>10300</td>
      <td>28300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2600</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2200</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>10200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>500</td>
      <td>0</td>
      <td>2300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2300</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10600</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM C10 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.13      1.00      0.24      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.81      0.82      0.81     81400
         Allaple.L       0.73      0.65      0.69     43300
     Alueron.gen!J       1.00      0.92      0.96      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.46      0.21      0.29      5300
       C2LOP.gen!g       0.11      0.03      0.04      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.95      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       0.97      0.80      0.88      4600
       Malex.gen!J       1.00      0.24      0.38      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.87      0.93      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.33      0.05      0.09      3700
     Swizzor.gen!I       0.50      0.11      0.18      3600
             VB.AT       0.99      0.95      0.97     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.80    256000
         macro avg       0.84      0.77      0.77    256000
      weighted avg       0.84      0.80      0.81    256000
    
    MLP-SVM C10 Accuracy : 0.8015625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='C0.5')
plt.plot(mlp_c10_steps, mlp_c10_accs, label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='C0.5')
plt.plot(mlp_c10_steps[2:], mlp_c10_accs[2:], label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_82_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_82_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='C0.5')
plt.plot(mlp_c10_steps, mlp_c10_losses, label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='C0.5')
plt.plot(mlp_c10_steps[2:], mlp_c10_losses[2:], label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_83_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_83_3.png)
    


# Additional Research

## Swish Activation Function

In this section, we replace the usage of LeakyReLU in CNN-SVM and MLP-SVM with the Swish activation function.

The reason is because, similar to the principles behind the GRU-SVM model, Swish is a self-gated activation function.

### CNN-SVM Model with Swish


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm_swish
!mkdir ./repr_cnn_svm_swish

from addons.cnn_svm_swish import CNN_Swish

tf.keras.backend.clear_session()
model = CNN_Swish(batch_size=BATCH_SIZE,
                  num_classes=num_classes,
                  sequence_length=num_features,
                  alpha=1e-3,
                  penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm_swish/checkpoints/',
            log_path='./repr_cnn_svm_swish/logs/',
            result_path='./repr_cnn_svm_swish/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
cnn_swish_steps, cnn_swish_losses, cnn_swish_accs = process_capture(model_type=1, # 1 <- CNN (same format)
                                                                    cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.12109375, training loss : 702.5133056640625
    step: 100, training accuracy : 0.77734375, training loss : 0.9894969463348389
    step: 200, training accuracy : 0.875, training loss : 0.5368702411651611
    step: 300, training accuracy : 0.9375, training loss : 0.3460381329059601
    step: 400, training accuracy : 0.96875, training loss : 0.23047754168510437
    step: 500, training accuracy : 0.96484375, training loss : 0.17061972618103027
    step: 600, training accuracy : 0.9921875, training loss : 0.1345679759979248
    step: 700, training accuracy : 0.99609375, training loss : 0.21255746483802795
    step: 800, training accuracy : 1.0, training loss : 0.07910345494747162
    step: 900, training accuracy : 1.0, training loss : 0.0379757322371006
    step: 1000, training accuracy : 1.0, training loss : 0.03223608061671257
    step: 1100, training accuracy : 1.0, training loss : 0.022772166877985
    step: 1200, training accuracy : 1.0, training loss : 0.07466653734445572
    step: 1300, training accuracy : 1.0, training loss : 0.15962238609790802
    step: 1400, training accuracy : 0.98046875, training loss : 0.1233152449131012
    step: 1500, training accuracy : 0.9765625, training loss : 0.19812247157096863
    step: 1600, training accuracy : 1.0, training loss : 0.028306322172284126
    step: 1700, training accuracy : 1.0, training loss : 0.021229352802038193
    step: 1800, training accuracy : 1.0, training loss : 0.012875916436314583
    step: 1900, training accuracy : 1.0, training loss : 0.010300157591700554
    step: 2000, training accuracy : 1.0, training loss : 0.01184976939111948
    step: 2100, training accuracy : 1.0, training loss : 0.009063470177352428
    step: 2200, training accuracy : 1.0, training loss : 0.008509008213877678
    step: 2300, training accuracy : 1.0, training loss : 0.008516985923051834
    step: 2400, training accuracy : 1.0, training loss : 0.008495737798511982
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 100, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 200, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 300, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 400, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 500, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 600, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 700, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 800, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    step: 900, testing accuracy : 0.80078125, testing loss : 0.9763079285621643
    EOF -- Testing done at step 999
    
    Completed after: 53.03s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM Swish', './repr_cnn_svm_swish/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_90_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM Swish', report))
print("{} Accuracy : {}".format('CNN-SVM Swish', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5700</td>
      <td>0</td>
      <td>65900</td>
      <td>9800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6800</td>
      <td>0</td>
      <td>8400</td>
      <td>28100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3100</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1900</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1000</td>
      <td>0</td>
      <td>2400</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1500</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1600</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>1000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11000</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Swish Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.12      1.00      0.22      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.83      0.81      0.82     81400
         Allaple.L       0.74      0.65      0.69     43300
     Alueron.gen!J       1.00      0.90      0.95      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.74      0.26      0.39      5300
       C2LOP.gen!g       0.71      0.14      0.23      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.97      0.99     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.98      0.98      0.98      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       0.97      0.83      0.89      4600
       Malex.gen!J       0.67      0.05      0.10      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.96      0.98      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.63      0.32      0.43      3700
     Swizzor.gen!I       0.53      0.28      0.36      3600
             VB.AT       0.99      0.98      0.99     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.81    256000
         macro avg       0.88      0.80      0.80    256000
      weighted avg       0.86      0.81      0.82    256000
    
    CNN-SVM Swish Accuracy : 0.8078125



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='LeakyReLU')
plt.plot(cnn_swish_steps, cnn_swish_accs, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='LeakyReLU')
plt.plot(cnn_swish_steps[3:], cnn_swish_accs[3:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_92_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_92_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='LeakyReLU')
plt.plot(cnn_swish_steps, cnn_swish_losses, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='LeakyReLU')
plt.plot(cnn_swish_steps[3:], cnn_swish_losses[3:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_93_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_93_3.png)
    


### MLP-SVM Model with Swish


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_swish
!mkdir ./repr_mlp_svm_swish

from addons.mlp_svm_swish import MLP_Swish

tf.keras.backend.clear_session()
model = MLP_Swish(batch_size=BATCH_SIZE,
                  num_classes=num_classes,
                  num_features=num_features,
                  node_size=[512, 256, 128],
                  alpha=1e-3,
                  penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_swish/checkpoints/',
            log_path='./repr_mlp_svm_swish/logs/',
            result_path='./repr_mlp_svm_swish/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_swish_steps, mlp_swish_losses, mlp_swish_accs = process_capture(model_type=3, # 3 <- MLP
                                                                    cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.014829374849796295, accuracy : 1.0
    step [200] train -- loss : 0.013729756698012352, accuracy : 0.9765625
    step [300] train -- loss : 0.007996536791324615, accuracy : 1.0
    step [400] train -- loss : 0.006461016368120909, accuracy : 1.0
    step [500] train -- loss : 0.005213703028857708, accuracy : 1.0
    step [600] train -- loss : 0.004217634443193674, accuracy : 1.0
    step [700] train -- loss : 0.003425345988944173, accuracy : 1.0
    step [800] train -- loss : 0.0028020725585520267, accuracy : 1.0
    step [900] train -- loss : 0.002300830325111747, accuracy : 1.0
    step [1000] train -- loss : 0.001906630932353437, accuracy : 1.0
    step [1100] train -- loss : 0.0015954035334289074, accuracy : 1.0
    step [1200] train -- loss : 0.0013419663300737739, accuracy : 1.0
    step [1300] train -- loss : 0.0011340142227709293, accuracy : 1.0
    step [1400] train -- loss : 0.000982648110948503, accuracy : 1.0
    step [1500] train -- loss : 0.0008823417010717094, accuracy : 1.0
    step [1600] train -- loss : 0.0007204069406725466, accuracy : 1.0
    step [1700] train -- loss : 0.0006211587460711598, accuracy : 1.0
    step [1800] train -- loss : 0.0005317622562870383, accuracy : 1.0
    step [1900] train -- loss : 0.0004674586234614253, accuracy : 1.0
    step [2000] train -- loss : 0.0004153857589699328, accuracy : 1.0
    step [2100] train -- loss : 0.00036132833338342607, accuracy : 1.0
    step [2200] train -- loss : 0.00031652499455958605, accuracy : 1.0
    step [2300] train -- loss : 0.00028370783547870815, accuracy : 1.0
    step [2400] train -- loss : 0.00032908699358813465, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [200] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [300] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [400] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [500] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [600] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [700] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [800] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    step [900] test -- loss : 0.03276560828089714, accuracy : 0.8203125
    EOF -- Testing done at step 999
    
    Completed after: 10.85s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM Swish', './repr_mlp_svm_swish/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_97_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM Swish', report))
print("{} Accuracy : {}".format('MLP-SVM Swish', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3600</td>
      <td>0</td>
      <td>69400</td>
      <td>8200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2900</td>
      <td>0</td>
      <td>12700</td>
      <td>27700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3100</td>
      <td>0</td>
      <td>900</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2300</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>500</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>900</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2000</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10700</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Swish Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.14      1.00      0.25      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.80      0.85      0.83     81400
         Allaple.L       0.77      0.64      0.70     43300
     Alueron.gen!J       0.98      0.96      0.97      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.43      0.19      0.26      5300
       C2LOP.gen!g       0.25      0.06      0.09      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.99      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.98      0.98      0.98      5700
        Lolyda.AA2       1.00      0.94      0.97      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.87      0.93      4600
       Malex.gen!J       0.92      0.29      0.44      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.87      0.93      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.50      0.14      0.21      3700
     Swizzor.gen!I       0.67      0.06      0.10      3600
             VB.AT       0.99      0.96      0.97     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.81    256000
         macro avg       0.86      0.78      0.78    256000
      weighted avg       0.85      0.81      0.82    256000
    
    MLP-SVM Swish Accuracy : 0.8125



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='LeakyReLU')
plt.plot(mlp_swish_steps, mlp_swish_accs, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='LeakyReLU')
plt.plot(mlp_swish_steps[2:], mlp_swish_accs[2:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_99_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_99_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='LeakyReLU')
plt.plot(mlp_swish_steps, mlp_swish_losses, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='LeakyReLU')
plt.plot(mlp_swish_steps[2:], mlp_swish_losses[2:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_100_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_100_3.png)
    

