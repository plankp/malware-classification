# Mini Project 4

Paper:
https://arxiv.org/pdf/1801.00318.pdf

Repository:
https://github.com/AFAgarap/malware-classification

Note that we maintain a fork here:
https://github.com/plankp/malware-classification and we will be using that one instead (in case the original repository is updated, causing the results to deviate from the original paper).

## Attack Plan

- Run with pre-trained weights
  - [x] Provide accuracy values
  - [x] Provide discussion of result
  - [x] Generate confusion matrix
- Visuals
  - [x] Epoch vs Accuracy (Figure 2 in original paper)
  - [x] Epoch vs Loss (not done in original paper)
  - [x] Variant where we cut of the first few epochs
- [x] Time the execution time
- Ablation study / Hyperparameter stuff
  - [x] Reduce GRU layers from 5 to 3
  - [x] Increase Regularization (SVM C) for MLP-SVM
- Additional research
  - [x] Use the [Swish](https://arxiv.org/abs/1710.05941v2) activation function

## Encoutered Difficulties

<ul>
  <li>Environment Setup</li>
  <ul>
    <li>Tensorflow Version</li>
    <p>We did not know about this <code>%tensorflow_version 1.x</code> thing, we took around half a day to figure out how to switch back to 1.15</p>
    <p>&rightarrow; Solution was to use <code>%tensorflow_version 1.x</code></p>
    <li>Setting up the repo</li>
    <p>Due to how Python imports modules, we actually had to clone into the current directory, and not into a directory nested in the current one</p>
    <p>&rightarrow; Solution make sure we pull into the working directory</p>
    <li>Dependencies Conflict</li>
    <p>For example, Colaboratory has a newer numpy version where <code>np.load</code> has a different default behaviour</p>
    <p>&rightarrow; Solution rewrite the affect code in the notebook directly</p>
    <li>Colaboratory Disconnects</li>
    <p>Seriously!</p>
    <p>&rightarrow; Solution <b><i>Keep calm and hit the "Reconnect" button</i></b></p>
  </ul>
  <li>Model</li>
  <ul>
    <li>Coding Convention</li>
    <p>For example, we see <code>penalty_parameter</code> for some models and <code>svm_c</code> for another, but they all mean the same thing</p>
    <p>&rightarrow; Solution learn the API and use the correct keyword arguments</p>
    <p>Log messages is another example of this. We see loss -- accuracy for one model and accuracy -- loss for another</p>
    <p>&rightarrow; Solution pass additional argument to process the logs accordingly</p>
    <li>Design Pattern</li>
    <p>Had to manually reset the global state of tensorflow before the model runs to prevent running into strange redefinition errors (from tensorflow)</p>
    <p>&rightarrow; Solution use <code>tf.keras.backend.clear_session()</code> to reset tensorflow's interal state</p>
    <li>Machine Learning Norm</li>
    <p>It's a bit odd to see the <b>train</b> method of a model also accepting test data.</p>
    <p>&rightarrow; Solution realize that there was another piece of code in the original repository that will collect the results and use that in conjunction</p>
  </ul>
</ul>

> **Notes**
>
> *  We are not sure about GRU due to lack of exposure, but the model appears to be correct.
> *  Tensorflow gives lots of warning messages (due to deprecated API usage)


## Setup Environment

To fetch the code:


```python
%%shell

# clone the code into the current directory
git init .
git remote add origin https://github.com/plankp/malware-classification
git pull origin master
```

    Initialized empty Git repository in /content/.git/
    remote: Enumerating objects: 476, done.[K
    remote: Counting objects: 100% (151/151), done.[K
    remote: Compressing objects: 100% (128/128), done.[K
    remote: Total 476 (delta 57), reused 82 (delta 18), pack-reused 325[K
    Receiving objects: 100% (476/476), 108.81 MiB | 36.00 MiB/s, done.
    Resolving deltas: 100% (206/206), done.
    From https://github.com/plankp/malware-classification
     * branch            master     -> FETCH_HEAD
     * [new branch]      master     -> origin/master





    



We also need to report the environment we use. Here we have tensorflow list all the devices we have access to.


```python
%tensorflow_version 1.x

import tensorflow as tf
print('Tensorflow Version:')
print(tf.version.VERSION)
print()

print('Devices:')
for dev in tf.config.experimental_list_devices():
  print(dev)
```

    TensorFlow 1.x selected.
    Tensorflow Version:
    1.15.2
    
    Devices:
    /job:localhost/replica:0/task:0/device:CPU:0
    /job:localhost/replica:0/task:0/device:XLA_CPU:0
    /job:localhost/replica:0/task:0/device:XLA_GPU:0
    /job:localhost/replica:0/task:0/device:GPU:0


As for CPU information:


```python
!cat /proc/cpuinfo
```

    processor	: 0
    vendor_id	: GenuineIntel
    cpu family	: 6
    model		: 79
    model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
    stepping	: 0
    microcode	: 0x1
    cpu MHz		: 2199.998
    cache size	: 56320 KB
    physical id	: 0
    siblings	: 2
    core id		: 0
    cpu cores	: 1
    apicid		: 0
    initial apicid	: 0
    fpu		: yes
    fpu_exception	: yes
    cpuid level	: 13
    wp		: yes
    flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
    bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
    bogomips	: 4399.99
    clflush size	: 64
    cache_alignment	: 64
    address sizes	: 46 bits physical, 48 bits virtual
    power management:
    
    processor	: 1
    vendor_id	: GenuineIntel
    cpu family	: 6
    model		: 79
    model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
    stepping	: 0
    microcode	: 0x1
    cpu MHz		: 2199.998
    cache size	: 56320 KB
    physical id	: 0
    siblings	: 2
    core id		: 0
    cpu cores	: 1
    apicid		: 1
    initial apicid	: 1
    fpu		: yes
    fpu_exception	: yes
    cpuid level	: 13
    wp		: yes
    flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
    bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa
    bogomips	: 4399.99
    clflush size	: 64
    cache_alignment	: 64
    address sizes	: 46 bits physical, 48 bits virtual
    power management:
    


And GPU information:

> **Note**
>
> This might change depending on how Colaboratory decides to allocate its resources. Over the past several runs, we have came across a Tesla T4, NVIDIA K80, ... etc.


```python
!nvidia-smi -L
```

    GPU 0: Tesla T4 (UUID: GPU-930ebfc4-7a72-9cfc-07b3-4cf6df3f9add)


# Code Patching

This section is only needed if any of the provided code uses (then-deprecated) now-removed features.

Note that `main.py` and `classifier.py` rely on outdated behavior of `np.load`, but since we do not use those files, patching is not needed.

# Process Malimg Dataset

In this section, we load the Malimg dataset provided by the author and split it into train and test instances using a 70-30 split â€“ the same scheme used by the author.

One modification we make is to pass the `random_state` keyword argument when splitting. That way we can guarantee the train and test data is the same everytime the notebook is executed.

some stuff that we need for presenting the results:


```python
import pandas as pd
import matplotlib.pyplot as plt
from utils.data import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from time import time

MALWARE_FAMILIES = [
    "Adialer.C",
    "Agent.FYI",
    "Allaple.A",
    "Allaple.L",
    "Alueron.gen!J",
    "Autorun.K",
    "C2LOP.P",
    "C2LOP.gen!g",
    "Dialplatform.B",
    "Dontovo.A",
    "Fakerean",
    "Instantaccess",
    "Lolyda.AA1",
    "Lolyda.AA2",
    "Lolyda.AA3",
    "Lolyda.AT",
    "Malex.gen!J",
    "Obfuscator.AD",
    "Rbot!gen",
    "Skintrim.N",
    "Swizzor.gen!E",
    "Swizzor.gen!I",
    "VB.AT",
    "Wintrim.BX",
    "Yuner.A",
]

print('DONE')
```

    DONE


load the dataset and present some statistics:


```python
import numpy as np
from sklearn.model_selection import train_test_split
from utils.data import load_data
from utils.data import one_hot_encode

BATCH_SIZE = 256

malimg_dataset = np.load('./dataset/malimg.npz', allow_pickle=True)
features, labels = load_data(dataset=malimg_dataset)

print(f'Total number of samples prior to split: {features.shape[0]}')
df = pd.DataFrame({
    'Occurrences': np.bincount(labels),
    'Percecntage': np.bincount(labels) / features.shape[0] * 100
}, index=MALWARE_FAMILIES)
display(df.T)
print('\nBar Chart:')
df.iloc[0:,0:1].plot.bar()
plt.legend([])
plt.ylabel('Occurrences')
plt.xlabel('Malware Families (Output Labels)')
plt.show()
print('\nPie Chart:')
plt.figure(figsize=(8, 8))
plt.pie(np.bincount(labels))
plt.legend(MALWARE_FAMILIES)
plt.show()
del df
print()

labels = one_hot_encode(labels=labels)
```

    Total number of samples prior to split: 9339



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Adialer.C</th>
      <th>Agent.FYI</th>
      <th>Allaple.A</th>
      <th>Allaple.L</th>
      <th>Alueron.gen!J</th>
      <th>Autorun.K</th>
      <th>C2LOP.P</th>
      <th>C2LOP.gen!g</th>
      <th>Dialplatform.B</th>
      <th>Dontovo.A</th>
      <th>Fakerean</th>
      <th>Instantaccess</th>
      <th>Lolyda.AA1</th>
      <th>Lolyda.AA2</th>
      <th>Lolyda.AA3</th>
      <th>Lolyda.AT</th>
      <th>Malex.gen!J</th>
      <th>Obfuscator.AD</th>
      <th>Rbot!gen</th>
      <th>Skintrim.N</th>
      <th>Swizzor.gen!E</th>
      <th>Swizzor.gen!I</th>
      <th>VB.AT</th>
      <th>Wintrim.BX</th>
      <th>Yuner.A</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Occurrences</th>
      <td>122.00000</td>
      <td>116.000000</td>
      <td>2949.000000</td>
      <td>1591.000000</td>
      <td>198.000000</td>
      <td>106.000000</td>
      <td>200.000000</td>
      <td>146.000000</td>
      <td>177.000000</td>
      <td>162.000000</td>
      <td>381.000000</td>
      <td>431.000000</td>
      <td>213.000000</td>
      <td>184.000000</td>
      <td>123.000000</td>
      <td>159.000000</td>
      <td>136.000000</td>
      <td>142.000000</td>
      <td>158.00000</td>
      <td>80.000000</td>
      <td>128.000000</td>
      <td>132.000000</td>
      <td>408.000000</td>
      <td>97.000000</td>
      <td>800.000000</td>
    </tr>
    <tr>
      <th>Percecntage</th>
      <td>1.30635</td>
      <td>1.242103</td>
      <td>31.577257</td>
      <td>17.036085</td>
      <td>2.120141</td>
      <td>1.135025</td>
      <td>2.141557</td>
      <td>1.563337</td>
      <td>1.895278</td>
      <td>1.734661</td>
      <td>4.079666</td>
      <td>4.615055</td>
      <td>2.280758</td>
      <td>1.970232</td>
      <td>1.317058</td>
      <td>1.702538</td>
      <td>1.456259</td>
      <td>1.520505</td>
      <td>1.69183</td>
      <td>0.856623</td>
      <td>1.370596</td>
      <td>1.413428</td>
      <td>4.368776</td>
      <td>1.038655</td>
      <td>8.566228</td>
    </tr>
  </tbody>
</table>
</div>


    
    Bar Chart:



    
![png](images/output_21_3.png)
    


    
    Pie Chart:



    
![png](images/output_21_5.png)
    


    


and we split the data:


```python
# Here we load the Malimg dataset and split it into train and test data using a
# 70-30 scheme (same scheme used in `main.py`)

num_features = features.shape[1]
num_classes = labels.shape[1]

train_features, test_features, train_labels, test_labels = train_test_split(
    features, labels, test_size=0.30, stratify=labels,
    random_state=1234   # <- added to have consistent splits
)

train_size = train_features.shape[0]
train_features = train_features[: train_size - (train_size % BATCH_SIZE)]
train_labels = train_labels[: train_size - (train_size % BATCH_SIZE)]
train_size = train_features.shape[0]

test_size = test_features.shape[0]
test_features = test_features[: test_size - (test_size % BATCH_SIZE)]
test_labels = test_labels[: test_size - (test_size % BATCH_SIZE)]
test_size = test_features.shape[0]

print(f'Number of features: {num_features}')
print(f'Number of classes:  {num_classes}')
print(f'Train size:         {train_size}')
print(f'Test size:          {test_size}')
```

    Number of features: 1024
    Number of classes:  25
    Train size:         6400
    Test size:          2560


# Reproduce Results

In this section, we will run the three models â€” CNN-SVM, GRU-SVM and MLP-SVM â€” against the splitted data. We expect to get performance similar to the ones presented in the report.

We will be using the pretrained models provided by the author as well as completely retraining the model with the configurations presented in the report.

## Pretrained Models

Since the original author has provided the pretrained models in the repository, we will be using those to see if we can get similar performance as the report.

Despite having obtained much higher accuracies, these values follow the same trend as the report â€” GRU > MLP > CNN.

We suspect this is due to the way the dataset is split. The author uses `train_test_split` but does not report the seed used for the randomizer state. With the seed we pick, we may be including some of the training instances from the partition boundaries obtained by the author.

This serves as one of the reasons for us to retrain all three models with the configurations presented in the report.

### Utilities


```python
def present_results(phase, predictions):
  predictions_ = np.reshape(predictions, (predictions.shape[0] // num_classes,
                                          num_classes))
  test_labels_ = None

  with tf.Session() as sess:
    predictions_ = sess.run(tf.argmax(predictions_, 1))
    test_labels_ = sess.run(tf.argmax(test_labels, 1))

  # get the confusion matrix based on the actual and predicted labels
  conf = confusion_matrix(y_true=test_labels_, y_pred=predictions_)

  # get the classification report on the actual and predicted labels
  report = classification_report(
      y_true=test_labels_, y_pred=predictions_, target_names=MALWARE_FAMILIES
  )

  # create a confusion matrix plot
  plt.imshow(conf, cmap=plt.cm.Greys, interpolation="nearest")

  # set the plot title
  plt.title("Confusion Matrix for {} Phase".format(phase))

  # legend of intensity for the plot
  plt.colorbar()

  tick_marks = np.arange(len(MALWARE_FAMILIES))
  plt.xticks(tick_marks, MALWARE_FAMILIES, rotation=45)
  plt.yticks(tick_marks, MALWARE_FAMILIES)

  plt.tight_layout()
  plt.ylabel("Actual label")
  plt.xlabel("Predicted label")

  # show the plot
  plt.show()

  return conf, report
```

### CNN-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()
predictions, accuracies = predict(dataset=[test_features, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  model=1,  # <- 1 is for CNN-SVM
                                  model_path='./trained-cnn-svm')

display(predictions)
display(np.mean(accuracies))
```

    WARNING:tensorflow:From /content/classifier.py:45: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.
    
    WARNING:tensorflow:From /content/classifier.py:45: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.
    
    WARNING:tensorflow:From /content/classifier.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    
    WARNING:tensorflow:From /content/classifier.py:98: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.
    
    INFO:tensorflow:Restoring parameters from ./trained-cnn-svm/CNN-SVM-2400
    Loaded trained model from ./trained-cnn-svm/CNN-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.942578125



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained CNN-SVM', predictions)
```


    
![png](images/output_32_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11</td>
      <td>0</td>
      <td>772</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12</td>
      <td>0</td>
      <td>76</td>
      <td>345</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>105</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>48</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>2</td>
      <td>0</td>
      <td>8</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>44</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>7</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>25</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>27</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>111</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>25</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.40      1.00      0.57        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.89      0.95      0.92       814
         Allaple.L       0.91      0.80      0.85       433
     Alueron.gen!J       0.98      1.00      0.99        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.98      0.81      0.89        53
       C2LOP.gen!g       0.97      0.86      0.91        36
    Dialplatform.B       1.00      1.00      1.00        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       1.00      0.98      0.99       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      1.00      1.00        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      0.91      0.95        46
       Malex.gen!J       1.00      0.68      0.81        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       0.98      0.98      0.98        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.89      0.68      0.77        37
     Swizzor.gen!I       0.90      0.75      0.82        36
             VB.AT       1.00      0.99      1.00       112
        Wintrim.BX       1.00      0.89      0.94        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.93      2560
         macro avg       0.96      0.93      0.94      2560
      weighted avg       0.94      0.93      0.93      2560
    


### GRU-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()

test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

predictions, accuracies = predict(dataset=[test_features_, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  cell_size=256,
                                  model=2,  # <- 2 is for GRU-SVM
                                  model_path='./trained-gru-svm')

display(predictions)
display(np.mean(accuracies))

del test_features_
```

    INFO:tensorflow:Restoring parameters from ./trained-gru-svm/GRU-SVM-2400
    Loaded trained model from ./trained-gru-svm/GRU-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.957421875



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained GRU-SVM', predictions)
```


    
![png](images/output_36_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>771</td>
      <td>38</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>32</td>
      <td>398</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>41</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>28</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>46</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>45</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>112</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>27</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.73      1.00      0.85        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.95      0.95      0.95       814
         Allaple.L       0.91      0.92      0.91       433
     Alueron.gen!J       0.98      1.00      0.99        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.91      0.77      0.84        53
       C2LOP.gen!g       0.93      0.78      0.85        36
    Dialplatform.B       0.98      1.00      0.99        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       0.98      0.99      0.99       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      0.98      0.99        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      1.00      1.00        46
       Malex.gen!J       0.95      0.97      0.96        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       1.00      1.00      1.00        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.89      0.84      0.86        37
     Swizzor.gen!I       0.89      0.92      0.90        36
             VB.AT       0.99      1.00      1.00       112
        Wintrim.BX       1.00      0.96      0.98        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.96      2560
         macro avg       0.96      0.96      0.96      2560
      weighted avg       0.96      0.96      0.96      2560
    


### MLP-SVM Model


```python
from classifier import predict

tf.keras.backend.clear_session()
predictions, accuracies = predict(dataset=[test_features, test_labels],
                                  size=test_size,
                                  batch_size=256,
                                  model=3,  # <- 3 is for MLP-SVM
                                  model_path='./trained-mlp-svm')

display(predictions)
display(np.mean(accuracies))
```

    INFO:tensorflow:Restoring parameters from ./trained-mlp-svm/MLP-SVM-2400
    Loaded trained model from ./trained-mlp-svm/MLP-SVM-2400



    array([-1., -1., -1., ...,  1., -1., -1.])



    0.94765625



```python
plt.figure(figsize=(10, 10))
conf, report = present_results('Pretrained MLP-SVM', predictions)
```


    
![png](images/output_40_0.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM Pretrained', report))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21</td>
      <td>0</td>
      <td>759</td>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10</td>
      <td>0</td>
      <td>27</td>
      <td>396</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>38</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>11</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>21</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>119</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>29</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>10</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>110</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>26</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>217</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Pretrained Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.29      1.00      0.46        33
         Agent.FYI       1.00      1.00      1.00        31
         Allaple.A       0.94      0.93      0.94       814
         Allaple.L       0.92      0.91      0.92       433
     Alueron.gen!J       0.98      0.96      0.97        49
         Autorun.K       1.00      1.00      1.00        30
           C2LOP.P       0.86      0.72      0.78        53
       C2LOP.gen!g       1.00      0.58      0.74        36
    Dialplatform.B       1.00      1.00      1.00        49
         Dontovo.A       1.00      1.00      1.00        47
          Fakerean       1.00      0.99      1.00       107
     Instantaccess       1.00      1.00      1.00       119
        Lolyda.AA1       1.00      0.98      0.99        57
        Lolyda.AA2       1.00      0.98      0.99        48
        Lolyda.AA3       1.00      1.00      1.00        35
         Lolyda.AT       1.00      0.93      0.97        46
       Malex.gen!J       1.00      0.76      0.87        38
     Obfuscator.AD       1.00      1.00      1.00        37
          Rbot!gen       1.00      0.93      0.97        45
        Skintrim.N       1.00      1.00      1.00        23
     Swizzor.gen!E       0.96      0.68      0.79        37
     Swizzor.gen!I       1.00      0.72      0.84        36
             VB.AT       1.00      0.98      0.99       112
        Wintrim.BX       1.00      0.93      0.96        28
           Yuner.A       1.00      1.00      1.00       217
    
          accuracy                           0.93      2560
         macro avg       0.96      0.92      0.93      2560
      weighted avg       0.96      0.93      0.94      2560
    


## Retrained Models

In this section, we will be retraining the three models, and see if we can get similar performance as the report.

The values are similar to the ones presented in the report.

### Utilities


```python
import re

def process_capture(cap, model_type):
  """
  Returns:
  (steps, losses, train-accuracies)
  """

  cap.show()

  train_steps = []
  train_loss = []
  train_acc = []

  pattern = None
  grp_step = None
  grp_loss = None
  grp_acc = None
  if model_type == 1:
    # step: 0, training accuracy : 0.0859375, training loss : 1936.3583984375
    grp_step = 1
    grp_loss = 3
    grp_acc = 2
    pattern = r'^step: (.*), training accuracy : (.*), training loss : (.*)$'
  else:
    # step [0] train -- loss : 9.729708671569824, accuracy : 0.359375
    grp_step = 1
    grp_loss = 2
    grp_acc = 3
    pattern = r'^step \[(.*)\] train -- loss : (.*), accuracy : (.*)$'


  for line in cap.stdout.splitlines():
    if line.startswith('step'):
      groups = re.search(pattern, line)
      train_steps.append(int(groups.group(grp_step)))
      train_loss.append(float(groups.group(grp_loss)))
      train_acc.append(float(groups.group(grp_acc)))
    elif line.startswith('EOF'):
      break

  return train_steps, train_loss, train_acc
```

### CNN-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm
!mkdir ./repr_cnn_svm

from models.cnn_svm import CNN

tf.keras.backend.clear_session()
model = CNN(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            sequence_length=num_features,
            alpha=1e-3,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm/checkpoints/',
            log_path='./repr_cnn_svm/logs/',
            result_path='./repr_cnn_svm/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```

    WARNING:tensorflow:From /content/models/cnn_svm.py:52: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:312: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:342: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
    WARNING:tensorflow:From /content/models/cnn_svm.py:109: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:111: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
    
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    WARNING:tensorflow:From /content/models/cnn_svm.py:124: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:172: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:178: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.
    
    WARNING:tensorflow:From /content/models/cnn_svm.py:180: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
    
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use standard file APIs to delete files with this prefix.



```python
cnn_steps, cnn_losses, cnn_accs = process_capture(model_type=1, # 1 <- CNN
                                                  cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.140625, training loss : 3632.546875
    step: 100, training accuracy : 0.6796875, training loss : 3.0217652320861816
    step: 200, training accuracy : 0.75, training loss : 1.7475178241729736
    step: 300, training accuracy : 0.7890625, training loss : 1.166166067123413
    step: 400, training accuracy : 0.79296875, training loss : 1.001596450805664
    step: 500, training accuracy : 0.82421875, training loss : 0.7854675650596619
    step: 600, training accuracy : 0.83984375, training loss : 0.6336288452148438
    step: 700, training accuracy : 0.90234375, training loss : 0.5388743877410889
    step: 800, training accuracy : 0.93359375, training loss : 0.4735752046108246
    step: 900, training accuracy : 0.953125, training loss : 0.38206449151039124
    step: 1000, training accuracy : 0.9609375, training loss : 0.3018464744091034
    step: 1100, training accuracy : 0.96484375, training loss : 0.26725396513938904
    step: 1200, training accuracy : 0.9765625, training loss : 0.25972312688827515
    step: 1300, training accuracy : 0.98046875, training loss : 0.244498148560524
    step: 1400, training accuracy : 0.98046875, training loss : 0.19878223538398743
    step: 1500, training accuracy : 0.99609375, training loss : 0.1780775785446167
    step: 1600, training accuracy : 0.98828125, training loss : 0.14170752465724945
    step: 1700, training accuracy : 0.96484375, training loss : 0.1276225596666336
    step: 1800, training accuracy : 0.91015625, training loss : 0.5965574383735657
    step: 1900, training accuracy : 1.0, training loss : 0.09819884598255157
    step: 2000, training accuracy : 1.0, training loss : 0.08375413715839386
    step: 2100, training accuracy : 1.0, training loss : 0.07552453130483627
    step: 2200, training accuracy : 1.0, training loss : 0.085411936044693
    step: 2300, training accuracy : 1.0, training loss : 0.11006554961204529
    step: 2400, training accuracy : 1.0, training loss : 0.04144373908638954
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 100, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 200, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 300, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 400, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 500, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 600, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 700, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 800, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    step: 900, testing accuracy : 0.77734375, testing loss : 1.0389879941940308
    EOF -- Testing done at step 999
    
    Completed after: 48.07s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM', './repr_cnn_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_49_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM', report))
print("{} Accuracy : {}".format('CNN-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8400</td>
      <td>0</td>
      <td>64000</td>
      <td>8800</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5100</td>
      <td>0</td>
      <td>13400</td>
      <td>24500</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2200</td>
      <td>0</td>
      <td>1000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>1800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2000</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>3000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>500</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>10200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.12      1.00      0.22      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.77      0.79      0.78     81400
         Allaple.L       0.73      0.57      0.64     43300
     Alueron.gen!J       1.00      0.92      0.96      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.38      0.34      0.36      5300
       C2LOP.gen!g       0.40      0.06      0.10      3600
    Dialplatform.B       0.98      1.00      0.99      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.99      0.95      0.97     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       0.92      0.76      0.83      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       0.97      1.00      0.99      3700
          Rbot!gen       0.93      0.84      0.88      4500
        Skintrim.N       0.96      0.96      0.96      2300
     Swizzor.gen!E       0.46      0.16      0.24      3700
     Swizzor.gen!I       0.50      0.19      0.28      3600
             VB.AT       0.99      0.91      0.95     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.78    256000
         macro avg       0.80      0.77      0.76    256000
      weighted avg       0.82      0.78      0.79    256000
    
    CNN-SVM Accuracy : 0.775390625


### GRU-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm
!mkdir ./repr_gru_svm

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=5,
               dropout_rate=0.85,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm/checkpoints/',
            log_path='./repr_gru_svm/logs/',
            result_path='./repr_gru_svm/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    WARNING:tensorflow:
    The TensorFlow contrib module will not be included in TensorFlow 2.0.
    For more information, please see:
      * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
      * https://github.com/tensorflow/addons
      * https://github.com/tensorflow/io (for I/O related ops)
    If you depend on functionality not listed there, please file an issue.
    
    WARNING:tensorflow:From /content/models/gru_svm.py:99: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
    WARNING:tensorflow:From /content/models/gru_svm.py:105: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
    WARNING:tensorflow:From /content/models/gru_svm.py:113: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `keras.layers.RNN(cell)`, which is equivalent to this API
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `layer.add_weight` method instead.
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    WARNING:tensorflow:From /content/models/gru_svm.py:121: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.
    
    WARNING:tensorflow:From /content/models/gru_svm.py:397: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.
    


    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm/results/training-* # we capture the output for this info

gru_steps, gru_losses, gru_accs = process_capture(model_type=2, # 2 <- GRU
                                                  cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.080479621887207, accuracy : 0.390625
    step [100] train -- loss : 0.7019731998443604, accuracy : 0.70703125
    step [200] train -- loss : 0.500779390335083, accuracy : 0.765625
    step [300] train -- loss : 0.405976265668869, accuracy : 0.859375
    step [400] train -- loss : 0.19283998012542725, accuracy : 0.95703125
    step [500] train -- loss : 0.1409241110086441, accuracy : 0.95703125
    step [600] train -- loss : 0.11006676405668259, accuracy : 0.9609375
    step [700] train -- loss : 0.060697123408317566, accuracy : 0.98828125
    step [800] train -- loss : 0.05547557771205902, accuracy : 0.98046875
    step [900] train -- loss : 0.04298931360244751, accuracy : 0.98828125
    step [1000] train -- loss : 0.021619919687509537, accuracy : 0.99609375
    step [1100] train -- loss : 0.03181629627943039, accuracy : 0.9921875
    step [1200] train -- loss : 0.020263396203517914, accuracy : 0.99609375
    step [1300] train -- loss : 0.01723770797252655, accuracy : 0.99609375
    step [1400] train -- loss : 0.02918056957423687, accuracy : 0.9921875
    step [1500] train -- loss : 0.015820639207959175, accuracy : 0.99609375
    step [1600] train -- loss : 0.01723502203822136, accuracy : 0.99609375
    step [1700] train -- loss : 0.021014392375946045, accuracy : 0.99609375
    step [1800] train -- loss : 0.017302602529525757, accuracy : 0.99609375
    step [1900] train -- loss : 0.012775246053934097, accuracy : 1.0
    step [2000] train -- loss : 0.03539818897843361, accuracy : 0.98828125
    step [2100] train -- loss : 0.014587944373488426, accuracy : 1.0
    step [2200] train -- loss : 0.015500775538384914, accuracy : 0.99609375
    step [2300] train -- loss : 0.012736495584249496, accuracy : 1.0
    step [2400] train -- loss : 0.026866348460316658, accuracy : 0.98828125
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [200] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [300] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [400] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [500] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [600] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [700] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [800] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    step [900] test -- loss : 0.8559027910232544, accuracy : 0.8359375
    EOF -- Testing done at step 999
    
    Completed after: 281.95s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('GRU-SVM', './repr_gru_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_54_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM', report))
print("{} Accuracy : {}".format('GRU-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100</td>
      <td>0</td>
      <td>70100</td>
      <td>11100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100</td>
      <td>0</td>
      <td>12300</td>
      <td>30900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>600</td>
      <td>0</td>
      <td>400</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>2200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>200</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>300</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>200</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>100</td>
      <td>0</td>
      <td>1400</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>2000</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.60      1.00      0.75      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.82      0.86      0.84     81400
         Allaple.L       0.73      0.71      0.72     43300
     Alueron.gen!J       0.92      0.98      0.95      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.76      0.42      0.54      5300
       C2LOP.gen!g       0.79      0.61      0.69      3600
    Dialplatform.B       0.94      1.00      0.97      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.99      0.98      0.99     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.98      0.98      0.98      5700
        Lolyda.AA2       0.98      0.96      0.97      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       1.00      0.85      0.92      4600
       Malex.gen!J       1.00      0.53      0.69      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.96      0.98      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.43      0.27      0.33      3700
     Swizzor.gen!I       0.40      0.56      0.47      3600
             VB.AT       0.97      1.00      0.99     11200
        Wintrim.BX       0.88      0.82      0.85      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.86    256000
         macro avg       0.89      0.86      0.86    256000
      weighted avg       0.86      0.86      0.86    256000
    
    GRU-SVM Accuracy : 0.858203125


### MLP-SVM Model


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm
!mkdir ./repr_mlp_svm

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-3,
            penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm/checkpoints/',
            log_path='./repr_mlp_svm/logs/',
            result_path='./repr_mlp_svm/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_steps, mlp_losses, mlp_accs = process_capture(model_type=3, # 3 <- MLP
                                                  cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.01790262944996357, accuracy : 0.98046875
    step [200] train -- loss : 0.019507693126797676, accuracy : 0.91015625
    step [300] train -- loss : 0.009877884760499, accuracy : 1.0
    step [400] train -- loss : 0.008332542143762112, accuracy : 1.0
    step [500] train -- loss : 0.0072031146846711636, accuracy : 1.0
    step [600] train -- loss : 0.006217573769390583, accuracy : 1.0
    step [700] train -- loss : 0.005364010110497475, accuracy : 1.0
    step [800] train -- loss : 0.004628732800483704, accuracy : 1.0
    step [900] train -- loss : 0.003999562468379736, accuracy : 1.0
    step [1000] train -- loss : 0.0034647665452212095, accuracy : 1.0
    step [1100] train -- loss : 0.0029982267878949642, accuracy : 1.0
    step [1200] train -- loss : 0.002627493580803275, accuracy : 1.0
    step [1300] train -- loss : 0.002274890663102269, accuracy : 1.0
    step [1400] train -- loss : 0.0021504033356904984, accuracy : 1.0
    step [1500] train -- loss : 0.0023714543785899878, accuracy : 1.0
    step [1600] train -- loss : 0.0021922956220805645, accuracy : 1.0
    step [1700] train -- loss : 0.0024451131466776133, accuracy : 0.99609375
    step [1800] train -- loss : 0.008993945084512234, accuracy : 1.0
    step [1900] train -- loss : 0.0017488363664597273, accuracy : 1.0
    step [2000] train -- loss : 0.001261943019926548, accuracy : 1.0
    step [2100] train -- loss : 0.0010444356594234705, accuracy : 1.0
    step [2200] train -- loss : 0.0008861987735144794, accuracy : 1.0
    step [2300] train -- loss : 0.0007623136625625193, accuracy : 1.0
    step [2400] train -- loss : 0.0006644399254582822, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [200] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [300] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [400] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [500] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [600] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [700] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [800] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    step [900] test -- loss : 0.03779402747750282, accuracy : 0.80078125
    EOF -- Testing done at step 999
    
    Completed after: 10.86s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM', './repr_mlp_svm/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_59_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM', report))
print("{} Accuracy : {}".format('MLP-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2200</td>
      <td>0</td>
      <td>69300</td>
      <td>9600</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2100</td>
      <td>0</td>
      <td>11500</td>
      <td>29700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2200</td>
      <td>0</td>
      <td>1700</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2200</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>2100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2700</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>900</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10800</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.16      0.97      0.28      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.80      0.85      0.83     81400
         Allaple.L       0.75      0.69      0.71     43300
     Alueron.gen!J       0.98      0.90      0.94      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.50      0.15      0.23      5300
       C2LOP.gen!g       0.50      0.06      0.10      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.97      0.99     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.94      0.97      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.87      0.93      4600
       Malex.gen!J       0.83      0.26      0.40      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.91      0.95      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.20      0.03      0.05      3700
     Swizzor.gen!I       0.69      0.25      0.37      3600
             VB.AT       0.95      0.96      0.96     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.82    256000
         macro avg       0.85      0.78      0.78    256000
      weighted avg       0.85      0.82      0.82    256000
    
    MLP-SVM Accuracy : 0.819921875


### Visuals

In this section, we present two graphs.

The first one is the same as _Figure 2_ presented in the original paper, which plots the epoch (time step) against training accuracy.

Our graph follows a similar trend as the one presented in the original paper where MLP-SVM stays on top, and GRU-SVM is sandwiched between MLP and CNN-SVM.


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='CNN-SVM')
plt.plot(gru_steps, gru_accs, label='GRU-SVM')
plt.plot(mlp_steps, mlp_accs, label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='CNN-SVM')
plt.plot(gru_steps[3:], gru_accs[3:], label='GRU-SVM')
plt.plot(mlp_steps[2:], mlp_accs[2:], label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_64_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_64_3.png)
    


The second one plots epoch against training loss.


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='CNN-SVM')
plt.plot(gru_steps, gru_losses, label='GRU-SVM')
plt.plot(mlp_steps, mlp_losses, label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='CNN-SVM')
plt.plot(gru_steps[3:], gru_losses[3:], label='GRU-SVM')
plt.plot(mlp_steps[2:], mlp_losses[2:], label='MLP-SVM')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_66_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_66_3.png)
    


# Ablation Studies and Hyperparameter Tuning

## GRU-SVM Model with 3 layers

One of the things to note is that the GRU-SVM model had the most amount of hidden layers (5 vs 2 for CNN-SVM and 3 for MLP-SVM).

The last paragraph of the discussion section suggests making CNN-SVM and MLP-SVM more complex may potentially bring their test performance closer to GRU-SVM.

Here we do the opposite, so we will be reducing the complexity of the GRU-SVM model such that it uses 3 layers instead of 5.


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_d3
!mkdir ./repr_gru_svm_d3

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=3,
               dropout_rate=0.85,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_d3/checkpoints/',
            log_path='./repr_gru_svm_d3/logs/',
            result_path='./repr_gru_svm_d3/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_d3/results/training-*  # we capture the output for this info

gru_d3_steps, gru_d3_losses, gru_d3_accs = process_capture(model_type=2, # 2 <- GRU
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.33218765258789, accuracy : 0.14453125
    step [100] train -- loss : 0.6158237457275391, accuracy : 0.73828125
    step [200] train -- loss : 0.468242883682251, accuracy : 0.79296875
    step [300] train -- loss : 0.2801438570022583, accuracy : 0.921875
    step [400] train -- loss : 0.20103636384010315, accuracy : 0.94140625
    step [500] train -- loss : 0.19983479380607605, accuracy : 0.93359375
    step [600] train -- loss : 0.08035534620285034, accuracy : 0.97265625
    step [700] train -- loss : 0.0582883283495903, accuracy : 0.98828125
    step [800] train -- loss : 0.03107079491019249, accuracy : 0.99609375
    step [900] train -- loss : 0.02851739712059498, accuracy : 0.9921875
    step [1000] train -- loss : 0.04142817109823227, accuracy : 0.9921875
    step [1100] train -- loss : 0.03558984026312828, accuracy : 0.99609375
    step [1200] train -- loss : 0.019137093797326088, accuracy : 0.99609375
    step [1300] train -- loss : 0.010202608071267605, accuracy : 1.0
    step [1400] train -- loss : 0.022354813292622566, accuracy : 0.99609375
    step [1500] train -- loss : 0.008896928280591965, accuracy : 1.0
    step [1600] train -- loss : 0.018463803455233574, accuracy : 1.0
    step [1700] train -- loss : 0.011413207277655602, accuracy : 1.0
    step [1800] train -- loss : 0.011739598587155342, accuracy : 1.0
    step [1900] train -- loss : 0.018187209963798523, accuracy : 0.99609375
    step [2000] train -- loss : 0.01717524603009224, accuracy : 0.99609375
    step [2100] train -- loss : 0.00988741498440504, accuracy : 1.0
    step [2200] train -- loss : 0.021530618891119957, accuracy : 0.9921875
    step [2300] train -- loss : 0.01649799384176731, accuracy : 0.99609375
    step [2400] train -- loss : 0.009055305272340775, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [200] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [300] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [400] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [500] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [600] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [700] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [800] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    step [900] test -- loss : 0.7798551917076111, accuracy : 0.8515625
    EOF -- Testing done at step 999
    
    Completed after: 187.85s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('3 Layer GRU-SVM', './repr_gru_svm_d3/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_72_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('3 Layer GRU-SVM', report))
print("{} Accuracy : {}".format('3 Layer GRU-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100</td>
      <td>0</td>
      <td>68200</td>
      <td>12500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>200</td>
      <td>0</td>
      <td>12000</td>
      <td>30900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>800</td>
      <td>0</td>
      <td>700</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>2000</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>400</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>1000</td>
      <td>1500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>200</td>
      <td>0</td>
      <td>1700</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>800</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>1400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    3 Layer GRU-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.50      1.00      0.67      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.82      0.84      0.83     81400
         Allaple.L       0.70      0.71      0.71     43300
     Alueron.gen!J       0.91      1.00      0.95      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.48      0.38      0.42      5300
       C2LOP.gen!g       0.75      0.42      0.54      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.99      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       0.96      0.93      0.95      4600
       Malex.gen!J       0.59      0.42      0.49      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.98      0.98      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.43      0.32      0.37      3700
     Swizzor.gen!I       0.50      0.39      0.44      3600
             VB.AT       0.98      1.00      0.99     11200
        Wintrim.BX       0.96      0.82      0.88      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.85    256000
         macro avg       0.86      0.84      0.85    256000
      weighted avg       0.85      0.85      0.84    256000
    
    3 Layer GRU-SVM Accuracy : 0.845703125



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_accs, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_accs, label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_accs[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_accs[3:], label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_74_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_74_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_losses, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_losses, label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_losses[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_losses[3:], label='3 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_75_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_75_3.png)
    


## MLP-SVM Model with Penalization Factor of 10

Another thing to note in the original paper is that the penalization factor (SVM C in _Table 2_) for the MLP-SVM model is much smaller compared to the two other models (0.5 vs 10).

Here we rerun the MLP-SVM model with the penalization factor of 10.


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_c10
!mkdir ./repr_mlp_svm_c10

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-3,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_c10/checkpoints/',
            log_path='./repr_mlp_svm_c10/logs/',
            result_path='./repr_mlp_svm_c10/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_c10_steps, mlp_c10_losses, mlp_c10_accs = process_capture(model_type=3, # 3 <- MLP
                                                              cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.11259870231151581, accuracy : 0.98828125
    step [200] train -- loss : 0.18078066408634186, accuracy : 1.0
    step [300] train -- loss : 0.04003223776817322, accuracy : 1.0
    step [400] train -- loss : 0.013635662384331226, accuracy : 1.0
    step [500] train -- loss : 0.01329902932047844, accuracy : 1.0
    step [600] train -- loss : 0.013130963779985905, accuracy : 1.0
    step [700] train -- loss : 0.012949344702064991, accuracy : 1.0
    step [800] train -- loss : 0.01275547593832016, accuracy : 1.0
    step [900] train -- loss : 0.01255078800022602, accuracy : 1.0
    step [1000] train -- loss : 0.012336751446127892, accuracy : 1.0
    step [1100] train -- loss : 0.012114344164729118, accuracy : 1.0
    step [1200] train -- loss : 0.011884719133377075, accuracy : 1.0
    step [1300] train -- loss : 0.011649146676063538, accuracy : 1.0
    step [1400] train -- loss : 0.011407829821109772, accuracy : 1.0
    step [1500] train -- loss : 0.01116226427257061, accuracy : 1.0
    step [1600] train -- loss : 0.01091243140399456, accuracy : 1.0
    step [1700] train -- loss : 0.01066022738814354, accuracy : 1.0
    step [1800] train -- loss : 0.010404410772025585, accuracy : 1.0
    step [1900] train -- loss : 0.010147321969270706, accuracy : 1.0
    step [2000] train -- loss : 0.009891847148537636, accuracy : 1.0
    step [2100] train -- loss : 0.009670369327068329, accuracy : 1.0
    step [2200] train -- loss : 0.009403831325471401, accuracy : 1.0
    step [2300] train -- loss : 0.009152751415967941, accuracy : 1.0
    step [2400] train -- loss : 0.008885000832378864, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [200] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [300] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [400] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [500] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [600] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [700] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [800] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    step [900] test -- loss : 0.6183673739433289, accuracy : 0.8203125
    EOF -- Testing done at step 999
    
    Completed after: 10.39s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM C10', './repr_mlp_svm_c10/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_80_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM C10', report))
print("{} Accuracy : {}".format('MLP-SVM C10', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4800</td>
      <td>0</td>
      <td>68100</td>
      <td>8300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3700</td>
      <td>0</td>
      <td>12100</td>
      <td>27400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4400</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2900</td>
      <td>0</td>
      <td>1300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2100</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>700</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>2300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2200</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>600</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10600</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM C10 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.14      1.00      0.24      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.80      0.84      0.82     81400
         Allaple.L       0.76      0.63      0.69     43300
     Alueron.gen!J       0.98      0.90      0.94      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.40      0.15      0.22      5300
       C2LOP.gen!g       0.12      0.03      0.05      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.94      0.97      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       0.97      0.83      0.89      4600
       Malex.gen!J       0.80      0.21      0.33      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.89      0.93      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.45      0.14      0.21      3700
     Swizzor.gen!I       0.60      0.17      0.26      3600
             VB.AT       0.97      0.95      0.96     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.80    256000
         macro avg       0.84      0.77      0.77    256000
      weighted avg       0.84      0.80      0.81    256000
    
    MLP-SVM C10 Accuracy : 0.803515625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='C0.5')
plt.plot(mlp_c10_steps, mlp_c10_accs, label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='C0.5')
plt.plot(mlp_c10_steps[2:], mlp_c10_accs[2:], label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_82_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_82_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='C0.5')
plt.plot(mlp_c10_steps, mlp_c10_losses, label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='C0.5')
plt.plot(mlp_c10_steps[2:], mlp_c10_losses[2:], label='C10')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_83_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_83_3.png)
    


## Dropout Rate Changes

### CNN-SVM Model with Dropout Rate of 0.4


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm_dp4
!mkdir ./repr_cnn_svm_dp4

from addons.cnn_svm_dp4 import CNN_DP4

tf.keras.backend.clear_session()
model = CNN_DP4(batch_size=BATCH_SIZE,
                num_classes=num_classes,
                sequence_length=num_features,
                alpha=1e-3,
                penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm_dp4/checkpoints/',
            log_path='./repr_cnn_svm_dp4/logs/',
            result_path='./repr_cnn_svm_dp4/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
cnn_dp4_steps, cnn_dp4_losses, cnn_dp4_accs = process_capture(model_type=1, # 1 <- CNN
                                                              cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.31640625, training loss : 3157.181884765625
    step: 100, training accuracy : 0.72265625, training loss : 4.80063009262085
    step: 200, training accuracy : 0.75390625, training loss : 2.832050323486328
    step: 300, training accuracy : 0.79296875, training loss : 1.9450860023498535
    step: 400, training accuracy : 0.8125, training loss : 1.683242917060852
    step: 500, training accuracy : 0.82421875, training loss : 1.4102797508239746
    step: 600, training accuracy : 0.83203125, training loss : 1.3175584077835083
    step: 700, training accuracy : 0.875, training loss : 1.0494158267974854
    step: 800, training accuracy : 0.859375, training loss : 1.0614360570907593
    step: 900, training accuracy : 0.890625, training loss : 0.9721341133117676
    step: 1000, training accuracy : 0.88671875, training loss : 0.7473331093788147
    step: 1100, training accuracy : 0.9140625, training loss : 0.7427968978881836
    step: 1200, training accuracy : 0.9140625, training loss : 0.7772954106330872
    step: 1300, training accuracy : 0.94140625, training loss : 0.693147599697113
    step: 1400, training accuracy : 0.97265625, training loss : 0.5235904455184937
    step: 1500, training accuracy : 0.953125, training loss : 0.48813319206237793
    step: 1600, training accuracy : 0.98828125, training loss : 0.503068208694458
    step: 1700, training accuracy : 0.9765625, training loss : 0.3756878674030304
    step: 1800, training accuracy : 0.99609375, training loss : 0.3524830639362335
    step: 1900, training accuracy : 0.99609375, training loss : 0.27149009704589844
    step: 2000, training accuracy : 0.99609375, training loss : 0.25851014256477356
    step: 2100, training accuracy : 1.0, training loss : 0.26897695660591125
    step: 2200, training accuracy : 1.0, training loss : 0.2429903745651245
    step: 2300, training accuracy : 0.99609375, training loss : 0.20899847149848938
    step: 2400, training accuracy : 0.99609375, training loss : 0.23511840403079987
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 100, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 200, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 300, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 400, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 500, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 600, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 700, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 800, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    step: 900, testing accuracy : 0.76953125, testing loss : 1.1295278072357178
    EOF -- Testing done at step 999
    
    Completed after: 49.85s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM Dropout 0.4', './repr_cnn_svm_dp4/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_88_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM Dropout 0.4', report))
print("{} Accuracy : {}".format('CNN-SVM Dropout 0.4', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2700</td>
      <td>0</td>
      <td>72700</td>
      <td>6000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2400</td>
      <td>0</td>
      <td>19900</td>
      <td>21000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2300</td>
      <td>0</td>
      <td>1900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2100</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>200</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>800</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>2800</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2500</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1800</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10700</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Dropout 0.4 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.16      1.00      0.28      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.73      0.89      0.80     81400
         Allaple.L       0.77      0.48      0.60     43300
     Alueron.gen!J       0.98      0.88      0.92      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.56      0.17      0.26      5300
       C2LOP.gen!g       0.25      0.03      0.05      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.98      0.96      0.97     10700
     Instantaccess       1.00      0.99      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.74      0.85      4600
       Malex.gen!J       1.00      0.05      0.10      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.91      0.95      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.75      0.16      0.27      3700
     Swizzor.gen!I       0.82      0.25      0.38      3600
             VB.AT       1.00      0.96      0.98     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.80    256000
         macro avg       0.88      0.77      0.77    256000
      weighted avg       0.84      0.80      0.79    256000
    
    CNN-SVM Dropout 0.4 Accuracy : 0.7953125



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='0.85')
plt.plot(cnn_dp4_steps, cnn_dp4_accs, label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='0.85')
plt.plot(cnn_dp4_steps[3:], cnn_dp4_accs[3:], label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_90_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_90_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='0.85')
plt.plot(cnn_dp4_steps, cnn_dp4_losses, label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='0.85')
plt.plot(cnn_dp4_steps[3:], cnn_dp4_losses[3:], label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_91_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_91_3.png)
    


### GRU-SVM Model with Dropout Rate of 0.4


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_dp4
!mkdir ./repr_gru_svm_dp4

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=5,
               dropout_rate=0.4,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_dp4/checkpoints/',
            log_path='./repr_gru_svm_dp4/logs/',
            result_path='./repr_gru_svm_dp4/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_dp4/results/training-* # we capture the output for this info

gru_dp4_steps, gru_dp4_losses, gru_dp4_accs = process_capture(model_type=2, # 2 <- GRU
                                                              cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.55273151397705, accuracy : 0.04296875
    step [100] train -- loss : 1.1169118881225586, accuracy : 0.58984375
    step [200] train -- loss : 0.8066500425338745, accuracy : 0.6796875
    step [300] train -- loss : 0.6998251676559448, accuracy : 0.72265625
    step [400] train -- loss : 0.653703510761261, accuracy : 0.7265625
    step [500] train -- loss : 0.6149124503135681, accuracy : 0.7421875
    step [600] train -- loss : 0.5381722450256348, accuracy : 0.7734375
    step [700] train -- loss : 0.5118453502655029, accuracy : 0.7734375
    step [800] train -- loss : 0.4983687698841095, accuracy : 0.78125
    step [900] train -- loss : 0.48523467779159546, accuracy : 0.8046875
    step [1000] train -- loss : 0.5161148309707642, accuracy : 0.765625
    step [1100] train -- loss : 0.42631253600120544, accuracy : 0.828125
    step [1200] train -- loss : 0.4564055800437927, accuracy : 0.8046875
    step [1300] train -- loss : 0.4119172692298889, accuracy : 0.83984375
    step [1400] train -- loss : 0.41241806745529175, accuracy : 0.84765625
    step [1500] train -- loss : 0.41119781136512756, accuracy : 0.8203125
    step [1600] train -- loss : 0.36034560203552246, accuracy : 0.84375
    step [1700] train -- loss : 0.4023185670375824, accuracy : 0.81640625
    step [1800] train -- loss : 0.3557705581188202, accuracy : 0.83984375
    step [1900] train -- loss : 0.4039587378501892, accuracy : 0.83984375
    step [2000] train -- loss : 0.36427780985832214, accuracy : 0.83984375
    step [2100] train -- loss : 0.339874804019928, accuracy : 0.875
    step [2200] train -- loss : 0.3132847249507904, accuracy : 0.875
    step [2300] train -- loss : 0.3319298326969147, accuracy : 0.86328125
    step [2400] train -- loss : 0.2863447368144989, accuracy : 0.88671875
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [200] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [300] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [400] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [500] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [600] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [700] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [800] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    step [900] test -- loss : 0.9078301787376404, accuracy : 0.68359375
    EOF -- Testing done at step 999
    
    Completed after: 282.50s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('GRU-SVM Dropout 0.4', './repr_gru_svm_dp4/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.


    /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, msg_start, len(result))



    
![png](images/output_95_2.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM Dropout 0.4', report))
print("{} Accuracy : {}".format('GRU-SVM Dropout 0.4', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5200</td>
      <td>0</td>
      <td>68500</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11600</td>
      <td>0</td>
      <td>19300</td>
      <td>4800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1700</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1000</td>
      <td>0</td>
      <td>1600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>300</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>2500</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>5300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>400</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1700</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11000</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM Dropout 0.4 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.12      1.00      0.22      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.74      0.84      0.79     81400
         Allaple.L       0.81      0.11      0.20     43300
     Alueron.gen!J       0.85      0.90      0.87      4900
         Autorun.K       0.00      0.00      0.00      3000
           C2LOP.P       1.00      0.08      0.14      5300
       C2LOP.gen!g       0.51      0.69      0.59      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.57      0.98      0.72     10700
     Instantaccess       0.97      1.00      0.98     11900
        Lolyda.AA1       1.00      0.93      0.96      5700
        Lolyda.AA2       1.00      0.81      0.90      4800
        Lolyda.AA3       0.53      1.00      0.69      3500
         Lolyda.AT       0.91      0.93      0.92      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       0.66      1.00      0.80      3700
          Rbot!gen       1.00      0.93      0.97      4500
        Skintrim.N       1.00      0.78      0.88      2300
     Swizzor.gen!E       0.00      0.00      0.00      3700
     Swizzor.gen!I       0.57      0.33      0.42      3600
             VB.AT       0.97      0.98      0.98     11200
        Wintrim.BX       0.33      0.79      0.47      2800
           Yuner.A       0.88      1.00      0.94     21700
    
          accuracy                           0.71    256000
         macro avg       0.70      0.72      0.66    256000
      weighted avg       0.77      0.71      0.67    256000
    
    GRU-SVM Dropout 0.4 Accuracy : 0.71015625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_accs, label='0.85')
plt.plot(gru_dp4_steps, gru_dp4_accs, label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_accs[3:], label='0.85')
plt.plot(gru_dp4_steps[3:], gru_dp4_accs[3:], label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_97_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_97_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_losses, label='0.85')
plt.plot(gru_dp4_steps, gru_dp4_losses, label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_losses[3:], label='0.85')
plt.plot(gru_dp4_steps[3:], gru_dp4_losses[3:], label='0.4')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_98_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_98_3.png)
    


## Learning Rate Changes

### CNN-SVM Model with Learning Rate of 0.00001 and 0.1

With 0.00001


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm_lr_low
!mkdir ./repr_cnn_svm_lr_low

from models.cnn_svm import CNN

tf.keras.backend.clear_session()
model = CNN(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            sequence_length=num_features,
            alpha=1e-5,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm_lr_low/checkpoints/',
            log_path='./repr_cnn_svm_lr_low/logs/',
            result_path='./repr_cnn_svm_lr_low/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
cnn_low_steps, cnn_low_losses, cnn_low_accs = process_capture(model_type=1, # 1 <- CNN (same format)
                                                              cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.0859375, training loss : 2288.555419921875
    step: 100, training accuracy : 0.3359375, training loss : 102.14966583251953
    step: 200, training accuracy : 0.38671875, training loss : 81.0046615600586
    step: 300, training accuracy : 0.4453125, training loss : 64.33529663085938
    step: 400, training accuracy : 0.47265625, training loss : 58.166015625
    step: 500, training accuracy : 0.53125, training loss : 43.6048698425293
    step: 600, training accuracy : 0.5625, training loss : 41.22211456298828
    step: 700, training accuracy : 0.58203125, training loss : 32.15571212768555
    step: 800, training accuracy : 0.59375, training loss : 28.574438095092773
    step: 900, training accuracy : 0.6015625, training loss : 27.603517532348633
    step: 1000, training accuracy : 0.609375, training loss : 24.473155975341797
    step: 1100, training accuracy : 0.6171875, training loss : 23.638816833496094
    step: 1200, training accuracy : 0.625, training loss : 19.665306091308594
    step: 1300, training accuracy : 0.62109375, training loss : 17.48543930053711
    step: 1400, training accuracy : 0.62890625, training loss : 17.37888526916504
    step: 1500, training accuracy : 0.640625, training loss : 15.27460765838623
    step: 1600, training accuracy : 0.65625, training loss : 13.319732666015625
    step: 1700, training accuracy : 0.65234375, training loss : 12.939456939697266
    step: 1800, training accuracy : 0.67578125, training loss : 11.498933792114258
    step: 1900, training accuracy : 0.68359375, training loss : 11.229751586914062
    step: 2000, training accuracy : 0.70703125, training loss : 9.173527717590332
    step: 2100, training accuracy : 0.7109375, training loss : 8.68028736114502
    step: 2200, training accuracy : 0.71484375, training loss : 8.314093589782715
    step: 2300, training accuracy : 0.734375, training loss : 8.031949043273926
    step: 2400, training accuracy : 0.734375, training loss : 7.272591590881348
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 100, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 200, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 300, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 400, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 500, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 600, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 700, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 800, testing accuracy : 0.546875, testing loss : 5.156038284301758
    step: 900, testing accuracy : 0.546875, testing loss : 5.156038284301758
    EOF -- Testing done at step 999
    
    Completed after: 50.21s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM 0.00001', './repr_cnn_svm_lr_low/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_104_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM 0.00001', report))
print("{} Accuracy : {}".format('CNN-SVM 0.00001', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20600</td>
      <td>0</td>
      <td>50000</td>
      <td>8500</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11500</td>
      <td>0</td>
      <td>17800</td>
      <td>12600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1500</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3000</td>
      <td>0</td>
      <td>1100</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1900</td>
      <td>0</td>
      <td>500</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>500</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1600</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>8400</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2400</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1200</td>
      <td>0</td>
      <td>2400</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1600</td>
      <td>0</td>
      <td>200</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2000</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2100</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>500</td>
      <td>400</td>
      <td>100</td>
      <td>6100</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1300</td>
      <td>0</td>
      <td>800</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM 0.00001 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.05      0.97      0.10      3300
         Agent.FYI       0.97      0.97      0.97      3100
         Allaple.A       0.68      0.61      0.64     81400
         Allaple.L       0.54      0.29      0.38     43300
     Alueron.gen!J       0.91      0.61      0.73      4900
         Autorun.K       0.00      0.00      0.00      3000
           C2LOP.P       0.09      0.04      0.05      5300
       C2LOP.gen!g       0.04      0.03      0.03      3600
    Dialplatform.B       1.00      0.92      0.96      4900
         Dontovo.A       0.96      1.00      0.98      4700
          Fakerean       0.89      0.79      0.84     10700
     Instantaccess       0.99      0.99      0.99     11900
        Lolyda.AA1       0.90      0.77      0.83      5700
        Lolyda.AA2       0.97      0.81      0.89      4800
        Lolyda.AA3       0.97      0.86      0.91      3500
         Lolyda.AT       0.65      0.24      0.35      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       1.00      0.97      0.99      3700
          Rbot!gen       0.74      0.44      0.56      4500
        Skintrim.N       0.54      0.30      0.39      2300
     Swizzor.gen!E       0.31      0.11      0.16      3700
     Swizzor.gen!I       0.12      0.03      0.05      3600
             VB.AT       0.79      0.54      0.65     11200
        Wintrim.BX       0.00      0.00      0.00      2800
           Yuner.A       0.88      1.00      0.94     21700
    
          accuracy                           0.58    256000
         macro avg       0.60      0.53      0.53    256000
      weighted avg       0.67      0.58      0.61    256000
    
    CNN-SVM 0.00001 Accuracy : 0.580078125


With 0.1


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm_lr_hi
!mkdir ./repr_cnn_svm_lr_hi

from models.cnn_svm import CNN

tf.keras.backend.clear_session()
model = CNN(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            sequence_length=num_features,
            alpha=1e-1,
            penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm_lr_hi/checkpoints/',
            log_path='./repr_cnn_svm_lr_hi/logs/',
            result_path='./repr_cnn_svm_lr_hi/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
cnn_hi_steps, cnn_hi_losses, cnn_hi_accs = process_capture(model_type=1, # 1 <- CNN (same format)
                                                           cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.01953125, training loss : 2674.47705078125
    step: 100, training accuracy : 0.4609375, training loss : 9925929.0
    step: 200, training accuracy : 0.484375, training loss : 3497187.5
    step: 300, training accuracy : 0.4921875, training loss : 1778032.5
    step: 400, training accuracy : 0.53515625, training loss : 1098397.625
    step: 500, training accuracy : 0.54296875, training loss : 862105.25
    step: 600, training accuracy : 0.5859375, training loss : 636392.375
    step: 700, training accuracy : 0.58203125, training loss : 399748.03125
    step: 800, training accuracy : 0.59375, training loss : 371662.78125
    step: 900, training accuracy : 0.60546875, training loss : 250376.53125
    step: 1000, training accuracy : 0.609375, training loss : 225493.578125
    step: 1100, training accuracy : 0.61328125, training loss : 227909.515625
    step: 1200, training accuracy : 0.625, training loss : 149872.984375
    step: 1300, training accuracy : 0.63671875, training loss : 214402.890625
    step: 1400, training accuracy : 0.62890625, training loss : 138169.03125
    step: 1500, training accuracy : 0.6640625, training loss : 146272.75
    step: 1600, training accuracy : 0.640625, training loss : 113396.9375
    step: 1700, training accuracy : 0.65234375, training loss : 97280.03125
    step: 1800, training accuracy : 0.64453125, training loss : 92097.9296875
    step: 1900, training accuracy : 0.6484375, training loss : 75901.609375
    step: 2000, training accuracy : 0.66015625, training loss : 78787.359375
    step: 2100, training accuracy : 0.65234375, training loss : 59859.4296875
    step: 2200, training accuracy : 0.66015625, training loss : 67289.46875
    step: 2300, training accuracy : 0.66796875, training loss : 48125.109375
    step: 2400, training accuracy : 0.67578125, training loss : 42097.46484375
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 100, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 200, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 300, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 400, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 500, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 600, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 700, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 800, testing accuracy : 0.5390625, testing loss : 49740.40625
    step: 900, testing accuracy : 0.5390625, testing loss : 49740.40625
    EOF -- Testing done at step 999
    
    Completed after: 50.22s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM 0.1', './repr_cnn_svm_lr_hi/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_109_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM 0.1', report))
print("{} Accuracy : {}".format('CNN-SVM 0.1', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>200</td>
      <td>2900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22700</td>
      <td>0</td>
      <td>52900</td>
      <td>2400</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16100</td>
      <td>0</td>
      <td>21700</td>
      <td>3800</td>
      <td>200</td>
      <td>0</td>
      <td>800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1400</td>
      <td>0</td>
      <td>2600</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3500</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>900</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2000</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>900</td>
      <td>0</td>
      <td>400</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>3900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1100</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>2700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1400</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>3600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1300</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>1000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>600</td>
      <td>300</td>
      <td>8000</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1600</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM 0.1 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.05      1.00      0.10      3300
         Agent.FYI       0.97      0.94      0.95      3100
         Allaple.A       0.65      0.65      0.65     81400
         Allaple.L       0.46      0.09      0.15     43300
     Alueron.gen!J       0.14      0.02      0.04      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.10      0.17      0.13      5300
       C2LOP.gen!g       0.50      0.14      0.22      3600
    Dialplatform.B       0.96      0.90      0.93      4900
         Dontovo.A       0.96      1.00      0.98      4700
          Fakerean       0.86      0.73      0.79     10700
     Instantaccess       0.99      0.80      0.88     11900
        Lolyda.AA1       0.79      0.77      0.78      5700
        Lolyda.AA2       0.95      0.81      0.88      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       0.75      0.59      0.66      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       1.00      0.97      0.99      3700
          Rbot!gen       0.95      0.91      0.93      4500
        Skintrim.N       1.00      0.48      0.65      2300
     Swizzor.gen!E       0.00      0.00      0.00      3700
     Swizzor.gen!I       0.50      0.08      0.14      3600
             VB.AT       0.96      0.71      0.82     11200
        Wintrim.BX       0.67      0.14      0.24      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.58    256000
         macro avg       0.69      0.59      0.59    256000
      weighted avg       0.68      0.58      0.59    256000
    
    CNN-SVM 0.1 Accuracy : 0.57578125


Comparison with 0.001


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='0.001')
plt.plot(cnn_low_steps, cnn_low_accs, label='0.00001')
plt.plot(cnn_hi_steps, cnn_hi_accs, label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='0.001')
plt.plot(cnn_low_steps[3:], cnn_low_accs[3:], label='0.00001')
plt.plot(cnn_hi_steps[3:], cnn_hi_accs[3:], label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_112_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_112_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='0.001')
plt.plot(cnn_low_steps, cnn_low_losses, label='0.00001')
plt.plot(cnn_hi_steps, cnn_hi_losses, label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='0.001')
plt.plot(cnn_low_steps[3:], cnn_low_losses[3:], label='0.00001')
plt.plot(cnn_hi_steps[3:], cnn_hi_losses[3:], label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_113_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_113_3.png)
    


### GRU-SVM Model with Learning Rate of 0.00001 and 0.1

With 0.00001


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_lo
!mkdir ./repr_gru_svm_lo

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=5,
               dropout_rate=0.85,
               alpha=1e-5,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_lo/checkpoints/',
            log_path='./repr_gru_svm_lo/logs/',
            result_path='./repr_gru_svm_lo/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_lo/results/training-*  # we capture the output for this info

gru_lo_steps, gru_lo_losses, gru_lo_accs = process_capture(model_type=2, # 2 <- GRU
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 11.98366641998291, accuracy : 0.04296875
    step [100] train -- loss : 2.6023943424224854, accuracy : 0.25
    step [200] train -- loss : 1.4939872026443481, accuracy : 0.37890625
    step [300] train -- loss : 1.3981232643127441, accuracy : 0.4140625
    step [400] train -- loss : 1.27852463722229, accuracy : 0.4609375
    step [500] train -- loss : 1.1840989589691162, accuracy : 0.49609375
    step [600] train -- loss : 1.1714998483657837, accuracy : 0.48828125
    step [700] train -- loss : 1.1078224182128906, accuracy : 0.51953125
    step [800] train -- loss : 1.0722485780715942, accuracy : 0.53515625
    step [900] train -- loss : 1.05820631980896, accuracy : 0.55078125
    step [1000] train -- loss : 1.0190595388412476, accuracy : 0.5546875
    step [1100] train -- loss : 0.9758521914482117, accuracy : 0.55859375
    step [1200] train -- loss : 0.9814088940620422, accuracy : 0.578125
    step [1300] train -- loss : 0.9374181032180786, accuracy : 0.5625
    step [1400] train -- loss : 0.9352170825004578, accuracy : 0.59765625
    step [1500] train -- loss : 0.9298456907272339, accuracy : 0.57421875
    step [1600] train -- loss : 0.8599987030029297, accuracy : 0.62109375
    step [1700] train -- loss : 0.8763595223426819, accuracy : 0.59375
    step [1800] train -- loss : 0.8578757047653198, accuracy : 0.6328125
    step [1900] train -- loss : 0.8398740887641907, accuracy : 0.62890625
    step [2000] train -- loss : 0.8127911686897278, accuracy : 0.65234375
    step [2100] train -- loss : 0.7896101474761963, accuracy : 0.63671875
    step [2200] train -- loss : 0.7517029643058777, accuracy : 0.65625
    step [2300] train -- loss : 0.761318027973175, accuracy : 0.6484375
    step [2400] train -- loss : 0.7570701837539673, accuracy : 0.65625
    EOF -- Training done at step 2499
    step [100] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [200] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [300] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [400] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [500] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [600] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [700] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [800] test -- loss : 1.125974178314209, accuracy : 0.6171875
    step [900] test -- loss : 1.125974178314209, accuracy : 0.6171875
    EOF -- Testing done at step 999
    
    Completed after: 281.06s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('GRU-SVM 0.00001', './repr_gru_svm_lo/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.


    /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, msg_start, len(result))



    
![png](images/output_118_2.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM 0.00001', report))
print("{} Accuracy : {}".format('GRU-SVM 0.00001', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5500</td>
      <td>300</td>
      <td>69900</td>
      <td>5000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2500</td>
      <td>300</td>
      <td>34300</td>
      <td>6100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4400</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3600</td>
      <td>0</td>
      <td>700</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1700</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>800</td>
      <td>0</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>11400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>700</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>200</td>
      <td>0</td>
      <td>3500</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1000</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>800</td>
      <td>0</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM 0.00001 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.10      1.00      0.18      3300
         Agent.FYI       0.69      1.00      0.82      3100
         Allaple.A       0.62      0.86      0.72     81400
         Allaple.L       0.54      0.14      0.22     43300
     Alueron.gen!J       0.00      0.00      0.00      4900
         Autorun.K       0.00      0.00      0.00      3000
           C2LOP.P       0.00      0.00      0.00      5300
       C2LOP.gen!g       0.00      0.00      0.00      3600
    Dialplatform.B       0.71      1.00      0.83      4900
         Dontovo.A       0.96      1.00      0.98      4700
          Fakerean       0.82      0.85      0.83     10700
     Instantaccess       1.00      0.96      0.98     11900
        Lolyda.AA1       0.84      0.89      0.86      5700
        Lolyda.AA2       0.94      0.92      0.93      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       0.93      0.59      0.72      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.11      0.20      4500
        Skintrim.N       1.00      0.09      0.16      2300
     Swizzor.gen!E       0.00      0.00      0.00      3700
     Swizzor.gen!I       0.00      0.00      0.00      3600
             VB.AT       0.74      0.99      0.85     11200
        Wintrim.BX       0.00      0.00      0.00      2800
           Yuner.A       0.88      1.00      0.94     21700
    
          accuracy                           0.65    256000
         macro avg       0.55      0.53      0.49    256000
      weighted avg       0.62      0.65      0.59    256000
    
    GRU-SVM 0.00001 Accuracy : 0.645703125


With 0.1


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_hi
!mkdir ./repr_gru_svm_hi

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=5,
               dropout_rate=0.85,
               alpha=1e-1,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_hi/checkpoints/',
            log_path='./repr_gru_svm_hi/logs/',
            result_path='./repr_gru_svm_hi/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_hi/results/training-*  # we capture the output for this info

gru_hi_steps, gru_hi_losses, gru_hi_accs = process_capture(model_type=2, # 2 <- GRU
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 35.238380432128906, accuracy : 0.18359375
    step [100] train -- loss : 5.666059970855713, accuracy : 0.23828125
    step [200] train -- loss : 3.168586015701294, accuracy : 0.23046875
    step [300] train -- loss : 4.88703727722168, accuracy : 0.05859375
    step [400] train -- loss : 9.59793758392334, accuracy : 0.24609375
    step [500] train -- loss : 7.461249828338623, accuracy : 0.09375
    step [600] train -- loss : 9.454216957092285, accuracy : 0.08203125
    step [700] train -- loss : 4.432485580444336, accuracy : 0.12890625
    step [800] train -- loss : 8.732057571411133, accuracy : 0.25390625
    step [900] train -- loss : 8.753724098205566, accuracy : 0.0234375
    step [1000] train -- loss : 14.779298782348633, accuracy : 0.13671875
    step [1100] train -- loss : 3.248843193054199, accuracy : 0.12109375
    step [1200] train -- loss : 5.91643762588501, accuracy : 0.11328125
    step [1300] train -- loss : 8.837963104248047, accuracy : 0.1015625
    step [1400] train -- loss : 6.435619831085205, accuracy : 0.16796875
    step [1500] train -- loss : 28.624692916870117, accuracy : 0.0390625
    step [1600] train -- loss : 8.425992965698242, accuracy : 0.171875
    step [1700] train -- loss : 4.638455390930176, accuracy : 0.08984375
    step [1800] train -- loss : 10.534462928771973, accuracy : 0.1015625
    step [1900] train -- loss : 11.769635200500488, accuracy : 0.05078125
    step [2000] train -- loss : 5.185283184051514, accuracy : 0.33203125
    step [2100] train -- loss : 16.350940704345703, accuracy : 0.10546875
    step [2200] train -- loss : 5.659280776977539, accuracy : 0.1640625
    step [2300] train -- loss : 3.7320468425750732, accuracy : 0.12890625
    step [2400] train -- loss : 5.251282215118408, accuracy : 0.02734375
    EOF -- Training done at step 2499
    step [100] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [200] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [300] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [400] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [500] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [600] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [700] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [800] test -- loss : 4.581185817718506, accuracy : 0.12890625
    step [900] test -- loss : 4.581185817718506, accuracy : 0.12890625
    EOF -- Testing done at step 999
    
    Completed after: 280.61s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('GRU-SVM 0.1', './repr_gru_svm_hi/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.


    /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, msg_start, len(result))



    
![png](images/output_123_2.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('GRU-SVM 0.1', report))
print("{} Accuracy : {}".format('GRU-SVM 0.1', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42900</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>14700</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
    </tr>
    <tr>
      <th>8</th>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7800</td>
    </tr>
    <tr>
      <th>11</th>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1500</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
    </tr>
    <tr>
      <th>15</th>
      <td>4500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2100</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
    </tr>
    <tr>
      <th>19</th>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2700</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2400</td>
    </tr>
    <tr>
      <th>22</th>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6900</td>
    </tr>
    <tr>
      <th>23</th>
      <td>900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1900</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    GRU-SVM 0.1 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.03      1.00      0.05      3300
         Agent.FYI       0.00      0.00      0.00      3100
         Allaple.A       0.00      0.00      0.00     81400
         Allaple.L       0.00      0.00      0.00     43300
     Alueron.gen!J       0.00      0.00      0.00      4900
         Autorun.K       0.00      0.00      0.00      3000
           C2LOP.P       0.00      0.00      0.00      5300
       C2LOP.gen!g       0.00      0.00      0.00      3600
    Dialplatform.B       0.00      0.00      0.00      4900
         Dontovo.A       0.00      0.00      0.00      4700
          Fakerean       0.00      0.00      0.00     10700
     Instantaccess       0.00      0.00      0.00     11900
        Lolyda.AA1       0.00      0.00      0.00      5700
        Lolyda.AA2       0.00      0.00      0.00      4800
        Lolyda.AA3       0.00      0.00      0.00      3500
         Lolyda.AT       0.00      0.00      0.00      4600
       Malex.gen!J       0.00      0.00      0.00      3800
     Obfuscator.AD       0.00      0.00      0.00      3700
          Rbot!gen       0.00      0.00      0.00      4500
        Skintrim.N       0.00      0.00      0.00      2300
     Swizzor.gen!E       0.00      0.00      0.00      3700
     Swizzor.gen!I       0.00      0.00      0.00      3600
             VB.AT       0.00      0.00      0.00     11200
        Wintrim.BX       0.00      0.00      0.00      2800
           Yuner.A       0.17      1.00      0.29     21700
    
          accuracy                           0.10    256000
         macro avg       0.01      0.08      0.01    256000
      weighted avg       0.01      0.10      0.03    256000
    
    GRU-SVM 0.1 Accuracy : 0.09765625


Compare with 0.001


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_accs, label='0.001')
plt.plot(gru_lo_steps, gru_lo_accs, label='0.00001')
plt.plot(gru_hi_steps, gru_hi_accs, label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_accs[3:], label='0.001')
plt.plot(gru_lo_steps[3:], gru_lo_accs[3:], label='0.00001')
plt.plot(gru_hi_steps[3:], gru_hi_accs[3:], label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_126_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_126_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_losses, label='0.001')
plt.plot(gru_lo_steps, gru_lo_losses, label='0.00001')
plt.plot(gru_hi_steps, gru_hi_losses, label='0.00001')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_losses[3:], label='0.001')
plt.plot(gru_lo_steps[3:], gru_lo_losses[3:], label='0.00001')
plt.plot(gru_hi_steps[3:], gru_hi_losses[3:], label='0.00001')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_127_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_127_3.png)
    


### MLP-SVM Model with Learning Rate of 0.00001 and 0.1

With 0.00001


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_lo
!mkdir ./repr_mlp_svm_lo

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-5,
            penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_lo/checkpoints/',
            log_path='./repr_mlp_svm_lo/logs/',
            result_path='./repr_mlp_svm_lo/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_lo_steps, mlp_lo_losses, mlp_lo_accs = process_capture(model_type=3, # 3 <- MLP
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.5261901021003723, accuracy : 0.203125
    step [200] train -- loss : 0.2711227536201477, accuracy : 0.25390625
    step [300] train -- loss : 0.15800070762634277, accuracy : 0.37890625
    step [400] train -- loss : 0.10712344199419022, accuracy : 0.4609375
    step [500] train -- loss : 0.08319547772407532, accuracy : 0.53125
    step [600] train -- loss : 0.07069432735443115, accuracy : 0.59375
    step [700] train -- loss : 0.06325467675924301, accuracy : 0.64453125
    step [800] train -- loss : 0.05819738283753395, accuracy : 0.67578125
    step [900] train -- loss : 0.05440639704465866, accuracy : 0.69140625
    step [1000] train -- loss : 0.05136174336075783, accuracy : 0.71875
    step [1100] train -- loss : 0.048814594745635986, accuracy : 0.7421875
    step [1200] train -- loss : 0.046610407531261444, accuracy : 0.76953125
    step [1300] train -- loss : 0.0446460098028183, accuracy : 0.78125
    step [1400] train -- loss : 0.04286637529730797, accuracy : 0.78125
    step [1500] train -- loss : 0.041242972016334534, accuracy : 0.79296875
    step [1600] train -- loss : 0.039757415652275085, accuracy : 0.80078125
    step [1700] train -- loss : 0.03838430717587471, accuracy : 0.8046875
    step [1800] train -- loss : 0.03710725158452988, accuracy : 0.82421875
    step [1900] train -- loss : 0.03590196743607521, accuracy : 0.8359375
    step [2000] train -- loss : 0.034760091453790665, accuracy : 0.84375
    step [2100] train -- loss : 0.03367777541279793, accuracy : 0.84765625
    step [2200] train -- loss : 0.03265269100666046, accuracy : 0.8515625
    step [2300] train -- loss : 0.031672023236751556, accuracy : 0.87109375
    step [2400] train -- loss : 0.03072654828429222, accuracy : 0.87890625
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [200] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [300] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [400] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [500] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [600] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [700] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [800] test -- loss : 0.04106102138757706, accuracy : 0.734375
    step [900] test -- loss : 0.04106102138757706, accuracy : 0.734375
    EOF -- Testing done at step 999
    
    Completed after: 10.76s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM 0.00001', './repr_mlp_svm_lo/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_132_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM 0.00001', report))
print("{} Accuracy : {}".format('MLP-SVM 0.00001', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11400</td>
      <td>0</td>
      <td>61600</td>
      <td>8000</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9400</td>
      <td>0</td>
      <td>13200</td>
      <td>20600</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3000</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2200</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>200</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2700</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>900</td>
      <td>0</td>
      <td>2600</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2600</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1900</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>300</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>8600</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>500</td>
      <td>0</td>
      <td>1000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM 0.00001 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.08      1.00      0.14      3300
         Agent.FYI       0.97      0.97      0.97      3100
         Allaple.A       0.74      0.76      0.75     81400
         Allaple.L       0.71      0.48      0.57     43300
     Alueron.gen!J       0.97      0.76      0.85      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.25      0.06      0.09      5300
       C2LOP.gen!g       0.00      0.00      0.00      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.94      0.97     10700
     Instantaccess       1.00      0.99      1.00     11900
        Lolyda.AA1       0.96      0.95      0.96      5700
        Lolyda.AA2       1.00      0.98      0.99      4800
        Lolyda.AA3       1.00      0.94      0.97      3500
         Lolyda.AT       1.00      0.39      0.56      4600
       Malex.gen!J       1.00      0.03      0.05      3800
     Obfuscator.AD       0.97      1.00      0.99      3700
          Rbot!gen       0.94      0.69      0.79      4500
        Skintrim.N       0.95      0.87      0.91      2300
     Swizzor.gen!E       0.29      0.05      0.09      3700
     Swizzor.gen!I       0.50      0.08      0.14      3600
             VB.AT       0.95      0.77      0.85     11200
        Wintrim.BX       0.92      0.43      0.59      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.72    256000
         macro avg       0.81      0.69      0.69    256000
      weighted avg       0.80      0.72      0.73    256000
    
    MLP-SVM 0.00001 Accuracy : 0.715234375


With 0.1


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_hi
!mkdir ./repr_mlp_svm_hi

from models.mlp_svm import MLP

tf.keras.backend.clear_session()
model = MLP(batch_size=BATCH_SIZE,
            num_classes=num_classes,
            num_features=num_features,
            node_size=[512, 256, 128],
            alpha=1e-1,
            penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_hi/checkpoints/',
            log_path='./repr_mlp_svm_hi/logs/',
            result_path='./repr_mlp_svm_hi/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_hi_steps, mlp_hi_losses, mlp_hi_accs = process_capture(model_type=3, # 3 <- MLP
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 28354.220703125, accuracy : 0.55859375
    step [200] train -- loss : 4467.03076171875, accuracy : 0.66796875
    step [300] train -- loss : 1882.97265625, accuracy : 0.703125
    step [400] train -- loss : 1017.0071411132812, accuracy : 0.71484375
    step [500] train -- loss : 637.9921264648438, accuracy : 0.7265625
    step [600] train -- loss : 431.84307861328125, accuracy : 0.7265625
    step [700] train -- loss : 313.6182861328125, accuracy : 0.73046875
    step [800] train -- loss : 243.5286102294922, accuracy : 0.7421875
    step [900] train -- loss : 194.5518341064453, accuracy : 0.74609375
    step [1000] train -- loss : 158.70413208007812, accuracy : 0.75
    step [1100] train -- loss : 137.07510375976562, accuracy : 0.74609375
    step [1200] train -- loss : 384049.5, accuracy : 0.55078125
    step [1300] train -- loss : 902.7295532226562, accuracy : 0.69921875
    step [1400] train -- loss : 363.410400390625, accuracy : 0.72265625
    step [1500] train -- loss : 203.40391540527344, accuracy : 0.7421875
    step [1600] train -- loss : 143.28421020507812, accuracy : 0.7578125
    step [1700] train -- loss : 106.65765380859375, accuracy : 0.765625
    step [1800] train -- loss : 84.01499938964844, accuracy : 0.7734375
    step [1900] train -- loss : 68.46199798583984, accuracy : 0.77734375
    step [2000] train -- loss : 57.00308609008789, accuracy : 0.78125
    step [2100] train -- loss : 47.89040756225586, accuracy : 0.7890625
    step [2200] train -- loss : 40.37051010131836, accuracy : 0.79296875
    step [2300] train -- loss : 34.047874450683594, accuracy : 0.80078125
    step [2400] train -- loss : 29.18772315979004, accuracy : 0.80078125
    EOF -- Training done at step 2499
    step [100] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [200] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [300] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [400] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [500] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [600] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [700] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [800] test -- loss : 829.4716796875, accuracy : 0.58203125
    step [900] test -- loss : 829.4716796875, accuracy : 0.58203125
    EOF -- Testing done at step 999
    
    Completed after: 11.63s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM 0.1', './repr_mlp_svm_hi/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_137_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM 0.1', report))
print("{} Accuracy : {}".format('MLP-SVM 0.1', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>200</td>
      <td>2800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26300</td>
      <td>0</td>
      <td>49100</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20700</td>
      <td>0</td>
      <td>12800</td>
      <td>8700</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>800</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2300</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1500</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1600</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>700</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1700</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1600</td>
      <td>0</td>
      <td>2000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1700</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1200</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>600</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>300</td>
      <td>400</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>900</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>100</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>300</td>
      <td>200</td>
      <td>600</td>
      <td>0</td>
      <td>100</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>7300</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>400</td>
      <td>300</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1600</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM 0.1 Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.05      0.91      0.09      3300
         Agent.FYI       0.70      0.90      0.79      3100
         Allaple.A       0.75      0.60      0.67     81400
         Allaple.L       0.67      0.20      0.31     43300
     Alueron.gen!J       0.74      0.71      0.73      4900
         Autorun.K       0.91      1.00      0.95      3000
           C2LOP.P       0.37      0.28      0.32      5300
       C2LOP.gen!g       0.06      0.03      0.04      3600
    Dialplatform.B       0.96      1.00      0.98      4900
         Dontovo.A       0.87      1.00      0.93      4700
          Fakerean       0.94      0.89      0.91     10700
     Instantaccess       0.98      1.00      0.99     11900
        Lolyda.AA1       0.84      0.81      0.82      5700
        Lolyda.AA2       0.74      0.67      0.70      4800
        Lolyda.AA3       0.80      0.94      0.87      3500
         Lolyda.AT       0.74      0.37      0.49      4600
       Malex.gen!J       0.33      0.05      0.09      3800
     Obfuscator.AD       0.97      1.00      0.99      3700
          Rbot!gen       0.63      0.38      0.47      4500
        Skintrim.N       0.93      0.57      0.70      2300
     Swizzor.gen!E       0.38      0.08      0.13      3700
     Swizzor.gen!I       0.10      0.06      0.07      3600
             VB.AT       0.81      0.65      0.72     11200
        Wintrim.BX       0.80      0.57      0.67      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.60    256000
         macro avg       0.68      0.63      0.62    256000
      weighted avg       0.74      0.60      0.64    256000
    
    MLP-SVM 0.1 Accuracy : 0.599609375


Compare with 0.001


```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='0.001')
plt.plot(mlp_lo_steps, mlp_lo_accs, label='0.00001')
plt.plot(mlp_hi_steps, mlp_hi_accs, label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='0.001')
plt.plot(mlp_lo_steps[2:], mlp_lo_accs[2:], label='0.00001')
plt.plot(mlp_hi_steps[2:], mlp_hi_accs[2:], label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_140_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_140_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='0.001')
plt.plot(mlp_lo_steps, mlp_lo_losses, label='0.00001')
plt.plot(mlp_hi_steps, mlp_hi_losses, label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='0.001')
plt.plot(mlp_lo_steps[2:], mlp_lo_losses[2:], label='0.00001')
plt.plot(mlp_hi_steps[2:], mlp_hi_losses[2:], label='0.1')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_141_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_141_3.png)
    


# Additional Research

## Swish Activation Function

In this section, we replace the usage of LeakyReLU in CNN-SVM and MLP-SVM with the Swish activation function.

The reason is because, similar to the principles behind the GRU-SVM model, Swish is a self-gated activation function.

### CNN-SVM Model with Swish


```python
%%capture cap --no-stderr

!rm -rf ./repr_cnn_svm_swish
!mkdir ./repr_cnn_svm_swish

from addons.cnn_svm_swish import CNN_Swish

tf.keras.backend.clear_session()
model = CNN_Swish(batch_size=BATCH_SIZE,
                  num_classes=num_classes,
                  sequence_length=num_features,
                  alpha=1e-3,
                  penalty_parameter=10)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_cnn_svm_swish/checkpoints/',
            log_path='./repr_cnn_svm_swish/logs/',
            result_path='./repr_cnn_svm_swish/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
cnn_swish_steps, cnn_swish_losses, cnn_swish_accs = process_capture(model_type=1, # 1 <- CNN (same format)
                                                                    cap=cap)
```

    
    <log> Building graph...</log>
    step: 0, training accuracy : 0.015625, training loss : 1130.78564453125
    step: 100, training accuracy : 0.73046875, training loss : 1.3401515483856201
    step: 200, training accuracy : 0.796875, training loss : 0.7132214307785034
    step: 300, training accuracy : 0.87109375, training loss : 0.4519137442111969
    step: 400, training accuracy : 0.9296875, training loss : 0.3362269699573517
    step: 500, training accuracy : 0.97265625, training loss : 0.23309020698070526
    step: 600, training accuracy : 0.9921875, training loss : 0.14528553187847137
    step: 700, training accuracy : 0.9921875, training loss : 0.10243599861860275
    step: 800, training accuracy : 0.99609375, training loss : 0.07165016978979111
    step: 900, training accuracy : 1.0, training loss : 0.05495841056108475
    step: 1000, training accuracy : 0.98046875, training loss : 0.2666826546192169
    step: 1100, training accuracy : 1.0, training loss : 0.04028936102986336
    step: 1200, training accuracy : 1.0, training loss : 0.041779860854148865
    step: 1300, training accuracy : 1.0, training loss : 0.022425465285778046
    step: 1400, training accuracy : 1.0, training loss : 0.032676003873348236
    step: 1500, training accuracy : 1.0, training loss : 0.058395642787218094
    step: 1600, training accuracy : 1.0, training loss : 0.08248740434646606
    step: 1700, training accuracy : 1.0, training loss : 0.1416577696800232
    step: 1800, training accuracy : 0.96484375, training loss : 0.20604480803012848
    step: 1900, training accuracy : 0.9765625, training loss : 0.17894373834133148
    step: 2000, training accuracy : 1.0, training loss : 0.03320448473095894
    step: 2100, training accuracy : 1.0, training loss : 0.02239220216870308
    step: 2200, training accuracy : 1.0, training loss : 0.014564421027898788
    step: 2300, training accuracy : 1.0, training loss : 0.01372167095541954
    step: 2400, training accuracy : 1.0, training loss : 0.014294055290520191
    EOF -- Training done at step 2499
    step: 0, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 100, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 200, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 300, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 400, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 500, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 600, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 700, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 800, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    step: 900, testing accuracy : 0.78515625, testing loss : 0.8774024844169617
    EOF -- Testing done at step 999
    
    Completed after: 55.64s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('CNN-SVM Swish', './repr_cnn_svm_swish/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_148_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('CNN-SVM Swish', report))
print("{} Accuracy : {}".format('CNN-SVM Swish', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5300</td>
      <td>0</td>
      <td>64100</td>
      <td>12000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4700</td>
      <td>0</td>
      <td>9600</td>
      <td>29000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3100</td>
      <td>0</td>
      <td>700</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2600</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>800</td>
      <td>0</td>
      <td>2500</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2700</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    CNN-SVM Swish Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.12      1.00      0.22      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.82      0.79      0.80     81400
         Allaple.L       0.70      0.67      0.68     43300
     Alueron.gen!J       1.00      0.94      0.97      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.73      0.21      0.32      5300
       C2LOP.gen!g       1.00      0.11      0.20      3600
    Dialplatform.B       0.98      1.00      0.99      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.98      0.99     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.98      0.98      0.98      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       1.00      0.72      0.84      4600
       Malex.gen!J       0.67      0.05      0.10      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.96      0.97      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.38      0.08      0.13      3700
     Swizzor.gen!I       0.67      0.22      0.33      3600
             VB.AT       0.99      0.99      0.99     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.80    256000
         macro avg       0.88      0.78      0.78    256000
      weighted avg       0.86      0.80      0.81    256000
    
    CNN-SVM Swish Accuracy : 0.79765625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_accs, label='LeakyReLU')
plt.plot(cnn_swish_steps, cnn_swish_accs, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_accs[3:], label='LeakyReLU')
plt.plot(cnn_swish_steps[3:], cnn_swish_accs[3:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_150_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_150_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps, cnn_losses, label='LeakyReLU')
plt.plot(cnn_swish_steps, cnn_swish_losses, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(cnn_steps[3:], cnn_losses[3:], label='LeakyReLU')
plt.plot(cnn_swish_steps[3:], cnn_swish_losses[3:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_151_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_151_3.png)
    


### MLP-SVM Model with Swish


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_swish
!mkdir ./repr_mlp_svm_swish

from addons.mlp_svm_swish import MLP_Swish

tf.keras.backend.clear_session()
model = MLP_Swish(batch_size=BATCH_SIZE,
                  num_classes=num_classes,
                  num_features=num_features,
                  node_size=[512, 256, 128],
                  alpha=1e-3,
                  penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_swish/checkpoints/',
            log_path='./repr_mlp_svm_swish/logs/',
            result_path='./repr_mlp_svm_swish/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_swish_steps, mlp_swish_losses, mlp_swish_accs = process_capture(model_type=3, # 3 <- MLP
                                                                    cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.014511242508888245, accuracy : 1.0
    step [200] train -- loss : 0.013814354315400124, accuracy : 0.984375
    step [300] train -- loss : 0.008642229251563549, accuracy : 1.0
    step [400] train -- loss : 0.007093492429703474, accuracy : 1.0
    step [500] train -- loss : 0.005790246184915304, accuracy : 1.0
    step [600] train -- loss : 0.004720041994005442, accuracy : 1.0
    step [700] train -- loss : 0.003855372779071331, accuracy : 1.0
    step [800] train -- loss : 0.003176012309268117, accuracy : 1.0
    step [900] train -- loss : 0.002604908077046275, accuracy : 1.0
    step [1000] train -- loss : 0.0021539863664656878, accuracy : 1.0
    step [1100] train -- loss : 0.0017825518734753132, accuracy : 1.0
    step [1200] train -- loss : 0.001574639230966568, accuracy : 1.0
    step [1300] train -- loss : 0.0012852533254772425, accuracy : 1.0
    step [1400] train -- loss : 0.001070118509232998, accuracy : 1.0
    step [1500] train -- loss : 0.0008996559772640467, accuracy : 1.0
    step [1600] train -- loss : 0.0007707909680902958, accuracy : 1.0
    step [1700] train -- loss : 0.000682843558024615, accuracy : 1.0
    step [1800] train -- loss : 0.0006006229086779058, accuracy : 1.0
    step [1900] train -- loss : 0.0005377164925448596, accuracy : 1.0
    step [2000] train -- loss : 0.00045616240822710097, accuracy : 1.0
    step [2100] train -- loss : 0.0003979558823630214, accuracy : 1.0
    step [2200] train -- loss : 0.0003439039282966405, accuracy : 1.0
    step [2300] train -- loss : 0.00030836454243399203, accuracy : 1.0
    step [2400] train -- loss : 0.00026781478663906455, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [200] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [300] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [400] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [500] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [600] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [700] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [800] test -- loss : 0.023364009335637093, accuracy : 0.796875
    step [900] test -- loss : 0.023364009335637093, accuracy : 0.796875
    EOF -- Testing done at step 999
    
    Completed after: 10.48s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM Swish', './repr_mlp_svm_swish/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_155_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM Swish', report))
print("{} Accuracy : {}".format('MLP-SVM Swish', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4100</td>
      <td>0</td>
      <td>68100</td>
      <td>9100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3600</td>
      <td>0</td>
      <td>9900</td>
      <td>29800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4400</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2900</td>
      <td>0</td>
      <td>1200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>900</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1900</td>
      <td>0</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>700</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>400</td>
      <td>0</td>
      <td>2000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2200</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>500</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10700</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM Swish Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.15      1.00      0.26      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.82      0.84      0.83     81400
         Allaple.L       0.76      0.69      0.72     43300
     Alueron.gen!J       1.00      0.90      0.95      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.39      0.17      0.24      5300
       C2LOP.gen!g       0.20      0.08      0.12      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.91      0.95      4600
       Malex.gen!J       0.93      0.34      0.50      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       1.00      0.93      0.97      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       1.00      0.14      0.24      3700
     Swizzor.gen!I       0.67      0.17      0.27      3600
             VB.AT       0.98      0.96      0.97     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.82    256000
         macro avg       0.88      0.79      0.79    256000
      weighted avg       0.86      0.82      0.83    256000
    
    MLP-SVM Swish Accuracy : 0.819140625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='LeakyReLU')
plt.plot(mlp_swish_steps, mlp_swish_accs, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='LeakyReLU')
plt.plot(mlp_swish_steps[2:], mlp_swish_accs[2:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_157_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_157_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='LeakyReLU')
plt.plot(mlp_swish_steps, mlp_swish_losses, label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='LeakyReLU')
plt.plot(mlp_swish_steps[2:], mlp_swish_losses[2:], label='Swish')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_158_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_158_3.png)
    


## Increase Hidden Layers

Another way to increase the complexity of a model is to increase the number of hidden layers.

### GRU-SVM Model with 7 Hidden Layers


```python
%%capture cap --no-stderr

!rm -rf ./repr_gru_svm_d7
!mkdir ./repr_gru_svm_d7

from models.gru_svm import GruSvm

tf.keras.backend.clear_session()

train_features_ = np.reshape(train_features, (train_size,
                                              int(np.sqrt(num_features)),
                                              int(np.sqrt(num_features))))
test_features_ = np.reshape(test_features, (test_size,
                                            int(np.sqrt(num_features)),
                                            int(np.sqrt(num_features))))

model = GruSvm(batch_size=BATCH_SIZE,
               num_classes=num_classes,
               sequence_width=train_features_.shape[1],
               sequence_height=train_features_.shape[2],
               cell_size=256,
               num_layers=3,
               dropout_rate=0.85,
               alpha=1e-3,
               svm_c=10)
start = time()
model.train(train_data=[train_features_, train_labels],
            train_size = train_size,
            test_data=[test_features_, test_labels],
            test_size=test_size,
            epochs=100,
            checkpoint_path='./repr_gru_svm_d7/checkpoints/',
            log_path='./repr_gru_svm_d7/logs/',
            result_path='./repr_gru_svm_d7/results/')
end = time()

# Free the reshaped data
del train_features_
del test_features_

print()
print(f'Completed after: {end - start:.2f}s')
```

    /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "



```python
!rm -rf ./repr_gru_svm_d7/results/training-*  # we capture the output for this info

gru_d7_steps, gru_d7_losses, gru_d7_accs = process_capture(model_type=2, # 2 <- GRU
                                                           cap=cap)
```

    
    <log> Building Graph...</log>
    step [0] train -- loss : 10.461700439453125, accuracy : 0.40234375
    step [100] train -- loss : 0.6122152805328369, accuracy : 0.74609375
    step [200] train -- loss : 0.4484580159187317, accuracy : 0.80078125
    step [300] train -- loss : 0.276329904794693, accuracy : 0.89453125
    step [400] train -- loss : 0.2072209119796753, accuracy : 0.95703125
    step [500] train -- loss : 0.14351755380630493, accuracy : 0.95703125
    step [600] train -- loss : 0.0367133803665638, accuracy : 0.99609375
    step [700] train -- loss : 0.04981042817234993, accuracy : 0.98828125
    step [800] train -- loss : 0.052715547382831573, accuracy : 0.984375
    step [900] train -- loss : 0.049335118383169174, accuracy : 0.98828125
    step [1000] train -- loss : 0.017509138211607933, accuracy : 0.99609375
    step [1100] train -- loss : 0.019513212144374847, accuracy : 0.99609375
    step [1200] train -- loss : 0.011162538081407547, accuracy : 1.0
    step [1300] train -- loss : 0.02251991629600525, accuracy : 0.98828125
    step [1400] train -- loss : 0.009968629106879234, accuracy : 1.0
    step [1500] train -- loss : 0.011300178244709969, accuracy : 1.0
    step [1600] train -- loss : 0.021626930683851242, accuracy : 0.99609375
    step [1700] train -- loss : 0.01373395137488842, accuracy : 1.0
    step [1800] train -- loss : 0.008477581664919853, accuracy : 1.0
    step [1900] train -- loss : 0.022303542122244835, accuracy : 0.9921875
    step [2000] train -- loss : 0.013111415319144726, accuracy : 1.0
    step [2100] train -- loss : 0.015592669136822224, accuracy : 0.99609375
    step [2200] train -- loss : 0.011921591125428677, accuracy : 1.0
    step [2300] train -- loss : 0.02279847487807274, accuracy : 0.99609375
    step [2400] train -- loss : 0.019015487283468246, accuracy : 0.99609375
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [200] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [300] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [400] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [500] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [600] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [700] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [800] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    step [900] test -- loss : 0.6427000761032104, accuracy : 0.8515625
    EOF -- Testing done at step 999
    
    Completed after: 186.04s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('7 Layer GRU-SVM', './repr_gru_svm_d7/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_164_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('7 Layer GRU-SVM', report))
print("{} Accuracy : {}".format('7 Layer GRU-SVM', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>800</td>
      <td>0</td>
      <td>69500</td>
      <td>10700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>400</td>
      <td>0</td>
      <td>13600</td>
      <td>29100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1000</td>
      <td>0</td>
      <td>500</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>2300</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>500</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>1400</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>100</td>
      <td>0</td>
      <td>1400</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1400</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>1400</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    7 Layer GRU-SVM Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.45      1.00      0.62      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.81      0.85      0.83     81400
         Allaple.L       0.73      0.67      0.70     43300
     Alueron.gen!J       0.94      1.00      0.97      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.66      0.43      0.52      5300
       C2LOP.gen!g       0.74      0.39      0.51      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       0.95      0.98      0.96     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       0.97      0.98      0.97      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      1.00      1.00      3500
         Lolyda.AT       0.95      0.91      0.93      4600
       Malex.gen!J       0.78      0.55      0.65      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.98      0.98      0.98      4500
        Skintrim.N       1.00      1.00      1.00      2300
     Swizzor.gen!E       0.50      0.38      0.43      3700
     Swizzor.gen!I       0.45      0.39      0.42      3600
             VB.AT       0.96      1.00      0.98     11200
        Wintrim.BX       0.96      0.82      0.88      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.85    256000
         macro avg       0.87      0.85      0.85    256000
      weighted avg       0.85      0.85      0.84    256000
    
    7 Layer GRU-SVM Accuracy : 0.84765625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_accs, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_accs, label='3 Layers')
plt.plot(gru_d7_steps, gru_d7_accs, label='7 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_accs[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_accs[3:], label='3 Layers')
plt.plot(gru_d7_steps[3:], gru_d7_accs[3:], label='7 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_166_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_166_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps, gru_losses, label='5 Layers')
plt.plot(gru_d3_steps, gru_d3_losses, label='3 Layers')
plt.plot(gru_d7_steps, gru_d7_losses, label='7 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(gru_steps[3:], gru_losses[3:], label='5 Layers')
plt.plot(gru_d3_steps[3:], gru_d3_losses[3:], label='3 Layers')
plt.plot(gru_d7_steps[3:], gru_d7_losses[3:], label='7 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_167_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_167_3.png)
    


### MLP-SVM Model with 5 Hidden Layers


```python
%%capture cap --no-stderr

!rm -rf ./repr_mlp_svm_five
!mkdir ./repr_mlp_svm_five

from addons.mlp_svm_five import MLP_Five

tf.keras.backend.clear_session()
model = MLP_Five(batch_size=BATCH_SIZE,
                 num_classes=num_classes,
                 num_features=num_features,
                 node_size=[512, 256, 128, 128, 128],
                 alpha=1e-3,
                 penalty_parameter=0.5)
start = time()
model.train(train_data=[train_features, train_labels],
            train_size = train_size,
            test_data=[test_features, test_labels],
            test_size=test_size,
            num_epochs=100,
            checkpoint_path='./repr_mlp_svm_five/checkpoints/',
            log_path='./repr_mlp_svm_five/logs/',
            result_path='./repr_mlp_svm_five/results/')
end = time()

print()
print(f'Completed after: {end - start:.2f}s')
```


```python
mlp_five_steps, mlp_five_losses, mlp_five_accs = process_capture(model_type=3, # 3 <- MLP
                                                                 cap=cap)
```

    
    <log> Building Graph...</log>
    step [100] train -- loss : 0.019374722614884377, accuracy : 0.9765625
    step [200] train -- loss : 0.012484854087233543, accuracy : 0.9921875
    step [300] train -- loss : 0.009216481819748878, accuracy : 1.0
    step [400] train -- loss : 0.007823953405022621, accuracy : 1.0
    step [500] train -- loss : 0.006609674077481031, accuracy : 1.0
    step [600] train -- loss : 0.005579574033617973, accuracy : 1.0
    step [700] train -- loss : 0.004710718058049679, accuracy : 1.0
    step [800] train -- loss : 0.00397977652028203, accuracy : 1.0
    step [900] train -- loss : 0.003366619348526001, accuracy : 1.0
    step [1000] train -- loss : 0.002853323705494404, accuracy : 1.0
    step [1100] train -- loss : 0.0024254857562482357, accuracy : 1.0
    step [1200] train -- loss : 0.002081365557387471, accuracy : 1.0
    step [1300] train -- loss : 0.0017779961926862597, accuracy : 1.0
    step [1400] train -- loss : 0.001587900216691196, accuracy : 1.0
    step [1500] train -- loss : 0.002391434507444501, accuracy : 1.0
    step [1600] train -- loss : 0.0013663661666214466, accuracy : 1.0
    step [1700] train -- loss : 0.0010966344270855188, accuracy : 1.0
    step [1800] train -- loss : 0.0009134794818237424, accuracy : 1.0
    step [1900] train -- loss : 0.0007711165817454457, accuracy : 1.0
    step [2000] train -- loss : 0.0006565134390257299, accuracy : 1.0
    step [2100] train -- loss : 0.0005628221551887691, accuracy : 1.0
    step [2200] train -- loss : 0.0004822330956812948, accuracy : 1.0
    step [2300] train -- loss : 0.0004240419948473573, accuracy : 1.0
    step [2400] train -- loss : 0.0003635901084635407, accuracy : 1.0
    EOF -- Training done at step 2499
    step [100] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [200] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [300] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [400] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [500] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [600] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [700] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [800] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    step [900] test -- loss : 0.028177330270409584, accuracy : 0.83984375
    EOF -- Testing done at step 999
    
    Completed after: 11.21s



```python
plt.figure(figsize=(10, 10))
conf, acc, report = plot_confusion_matrix('MLP-SVM 5 Layers', './repr_mlp_svm_five/results', MALWARE_FAMILIES)
```

    Done appending 0.0% of 1000
    Done appending 20.0% of 1000
    Done appending 40.0% of 1000
    Done appending 80.0% of 1000
    Done appending NPY files.



    
![png](images/output_171_1.png)
    



```python
print('Confusion Matrix Values:')
display(pd.DataFrame(conf, index=range(25), columns=range(25)))
print()
print("{} Classification report :\n{}".format('MLP-SVM 5 Layers', report))
print("{} Accuracy : {}".format('MLP-SVM 5 Layers', acc))
```

    Confusion Matrix Values:



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2200</td>
      <td>0</td>
      <td>67600</td>
      <td>11200</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1500</td>
      <td>0</td>
      <td>10100</td>
      <td>31700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>4300</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1800</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1600</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1800</td>
      <td>0</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>800</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>200</td>
      <td>0</td>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>600</td>
      <td>0</td>
      <td>2000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3800</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2200</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1700</td>
      <td>0</td>
      <td>300</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>600</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>300</td>
      <td>700</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1500</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>300</td>
      <td>1200</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>300</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10800</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>100</td>
      <td>0</td>
      <td>400</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21700</td>
    </tr>
  </tbody>
</table>
</div>


    
    MLP-SVM 5 Layers Classification report :
                    precision    recall  f1-score   support
    
         Adialer.C       0.20      1.00      0.33      3300
         Agent.FYI       1.00      1.00      1.00      3100
         Allaple.A       0.82      0.83      0.82     81400
         Allaple.L       0.74      0.73      0.73     43300
     Alueron.gen!J       0.98      0.88      0.92      4900
         Autorun.K       1.00      1.00      1.00      3000
           C2LOP.P       0.53      0.30      0.39      5300
       C2LOP.gen!g       0.08      0.03      0.04      3600
    Dialplatform.B       1.00      1.00      1.00      4900
         Dontovo.A       1.00      1.00      1.00      4700
          Fakerean       1.00      0.96      0.98     10700
     Instantaccess       1.00      1.00      1.00     11900
        Lolyda.AA1       1.00      0.98      0.99      5700
        Lolyda.AA2       1.00      0.96      0.98      4800
        Lolyda.AA3       1.00      0.97      0.99      3500
         Lolyda.AT       1.00      0.89      0.94      4600
       Malex.gen!J       0.85      0.29      0.43      3800
     Obfuscator.AD       1.00      1.00      1.00      3700
          Rbot!gen       0.97      0.84      0.90      4500
        Skintrim.N       1.00      0.96      0.98      2300
     Swizzor.gen!E       0.43      0.08      0.14      3700
     Swizzor.gen!I       0.50      0.33      0.40      3600
             VB.AT       0.98      0.96      0.97     11200
        Wintrim.BX       1.00      0.82      0.90      2800
           Yuner.A       1.00      1.00      1.00     21700
    
          accuracy                           0.83    256000
         macro avg       0.84      0.79      0.79    256000
      weighted avg       0.85      0.83      0.83    256000
    
    MLP-SVM 5 Layers Accuracy : 0.825390625



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_accs, label='3 Layers')
plt.plot(mlp_five_steps, mlp_five_accs, label='5 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_accs[2:], label='3 Layers')
plt.plot(mlp_five_steps[2:], mlp_five_accs[2:], label='5 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Accuracy')
plt.show()
```

    Full Chart



    
![png](images/output_173_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_173_3.png)
    



```python
print('Full Chart')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps, mlp_losses, label='3 Layers')
plt.plot(mlp_five_steps, mlp_five_losses, label='5 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
print()

print('Omitting the first few time steps')
plt.figure(figsize=(14, 8))
plt.plot(mlp_steps[2:], mlp_losses[2:], label='3 Layers')
plt.plot(mlp_five_steps[2:], mlp_five_losses[2:], label='5 Layers')
plt.legend()
plt.grid()
plt.xlabel('Time Step')
plt.ylabel('Training Loss')
plt.show()
```

    Full Chart



    
![png](images/output_174_1.png)
    


    
    Omitting the first few time steps



    
![png](images/output_174_3.png)
    

